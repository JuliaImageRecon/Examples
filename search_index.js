var documenterSearchIndex = {"docs":
[{"location":"generated/mri/95-varpro1/#varpro1","page":"Variable Projection: One exponential","title":"Variable Projection: One exponential","text":"Illustrate fitting one exponential to data.\n\nSee:\n\nVarPro blog\nVP4Optim.jl has biexponential fits\nVarpro.jl\n\nThis page comes from a single Julia file: 95-varpro1.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 95-varpro1.ipynb, or open it in binder here: 95-varpro1.ipynb.","category":"section"},{"location":"generated/mri/95-varpro1/#Setup","page":"Variable Projection: One exponential","title":"Setup","text":"Packages needed here.\n\nusing ImagePhantoms: ellipse, ellipse_parameters, phantom, SheppLoganBrainWeb\nusing Statistics: mean, std\nusing Plots: default, gui, histogram, plot, plot!, scatter, scatter!\nusing Plots: cgrad, RGB\ndefault(markerstrokecolor=:auto, label=\"\", widen = true)\nusing LaTeXStrings\nusing LinearAlgebra: norm, Diagonal, diag, diagm\nusing MIRTjim: jim\nusing Random: seed!; seed!(0)\nusing Unitful: @u_str, uconvert, ustrip, ms, s, mm\nusing InteractiveUtils: versioninfo","category":"section"},{"location":"generated/mri/95-varpro1/#Single-exponential","page":"Variable Projection: One exponential","title":"Single exponential","text":"We explore a simple case: fitting a single exponential to some noisy data: y_m = x e^- r t_m + ϵ_m for m = 1M. The two unknown parameters here are:\n\nthe decay rate r  0\nthe amplitude x (that could be complex in some MRI settings)\n\nTf = Float32\nTc = Complex{Tf}\nM = 8 # how many samples\nΔte = 25ms # echo spacing\nte1 = 5ms # time of first echo\ntm = Tf.(te1 .+ (0:(M-1)) * Δte) # echo times\nx_true = 100 # AU\nr_true = 20/s\nsignal(x, r; t=tm) = x * exp.(-t * r) # signal model\ny_true = signal(x_true, r_true)\ntf = range(0, M, 201) * Δte\nxaxis_t = (\"t\", (0,200).*ms, [0ms; tm; M*Δte])\npy = plot( xaxis = xaxis_t )\nplot!(py, tf, signal(x_true, r_true; t=tf), color=:black)\nscatter!(py, tm, y_true, label = \"Noiseless data, M=$M samples\")","category":"section"},{"location":"generated/mri/95-varpro1/#Random-phase-and-noise","page":"Variable Projection: One exponential","title":"Random phase and noise","text":"Actual MRI data has some phase and noise.\n\nphase_true = rand() * 2π + 0π\ny_true_phased = Tc.(cis(phase_true) * y_true)\n\nsnr = 25 # dB\nsnr2sigma(db, y) = 10^(-db/20) * norm(y) / sqrt(length(y))\nσ = Tf(snr2sigma(snr, y_true_phased))\nyc = y_true_phased + σ * randn(Tc, M)\n@show 20 * log10(norm(yc) / norm(yc - y_true_phased)) # check σ\n\npp = scatter(tm, angle.(y_true_phased), label = \"True data\",\n xaxis = xaxis_t,\n yaxis = (\"Phase\", (-π, π), ((-1:1)*π, [\"-π\", \"0\", \"π\"])),\n)\nscatter!(tm, angle.(yc), label=\"Noisy data\")\n\nThe phase of the noisy data becomes unreliable for low signal values\n\npc = scatter(tm, real(yc),\n label = \"Noisy data - real part\",\n xaxis = xaxis_t,\n ylim = (-100, 100),\n)\nscatter!(pc, tm, imag(yc),\n label = \"Noisy data - imag part\",\n)","category":"section"},{"location":"generated/mri/95-varpro1/#Phase-correction","page":"Variable Projection: One exponential","title":"Phase correction","text":"Phase correct signal using phase of first (noisy) data point\n\nyr = conj(sign(yc[1])) .* yc\n\npr = deepcopy(py)\nscatter!(pr, tm, real(yr),\n label = \"Phase corrected data - real part\",\n xaxis = xaxis_t,\n ylim = (-5, 105),\n marker = :square,\n)\nscatter!(pr, tm, imag(yr),\n label = \"Phase corrected data - imag part\",\n)\n\nExamine the distribution of real part after phase correction\n\nfunction make1_phase_corrected_signal()\n    phase_true = rand() * 2π\n    y_true_phased = Tc.(cis(phase_true) * y_true)\n    yc = y_true_phased + σ * randn(Tc, M)\n    yr = conj(sign(yc[1])) .* yc\nend\n\nN = 2000\nysim = stack(_ -> make1_phase_corrected_signal(), 1:N)\ntmp = ysim[end,:]\n\npe = scatter(real(tmp), imag(tmp), aspect_ratio=1,\n xaxis = (\"real(y_$M)\", (-4,8), -3:8),\n yaxis = (\"imag(y_$M)\", (-6,6), -5:5),\n)\nplot!(pe, real(y_true[end]) * [1,1], [-5, 5])\nplot!(pe, [-5, 5] .+ 3, imag(y_true[end]) * [1,1])\n\nHistogram of the real part looks reasonably Gaussian\n\nph = histogram((real(tmp) .- real(y_true[end])) / (σ/√2), bins=-4:0.1:4,\n xlabel = \"Real part of phase-corrected signal y_$M\")","category":"section"},{"location":"generated/mri/95-varpro1/#Log-linear-fit","page":"Variable Projection: One exponential","title":"Log-linear fit","text":"LS fitting using log of absolute value of real part of phase-corrected data.\n\nyl = @. log(abs(yr))\npl = plot(xaxis=xaxis_t, yaxis=(\"Log data\"))\nlog_fine = log.(signal(x_true, r_true; t=tf))\nplot!(pl, tf, log_fine, color=:lightgray)\nscatter!(pl, tm, log.(y_true), label=\"True\", color=:black)\nscatter!(pl, tm, yl, label=\"Noisy\", color=:red)\n\nLinear (not affine!) fit, after normalizing data by 1st data point (which is a bit smaller than x_true since first sample is not at t=0).\n\ntm_diff = tm .- te1 # time shift by 1st echo\nA1 = reshape(-tm_diff, M, 1)\nyl0 = yl .- yl[1] # normalize\nA1pinv = inv(A1'*A1) * A1'\nr1 = A1pinv * yl0\nr1 = only(r1)\n\nTs = u\"s^-1\"\nxaxis_td = (\"Δt\", (0,195).*ms, [0ms; tm_diff; M*Δte])\npf1 = plot(xaxis=xaxis_td, yaxis=(\"Log data\"))\nplot!(pf1, tf .- te1, log_fine .- log(y_true[1]), color=:lightgray)\nscatter!(pf1, tm_diff, log.(y_true/y_true[1]),\n label=\"True for R1=$r_true\", color=:black)\nscatter!(pf1, tm_diff, yl0, label = \"Noisy\", color=:red)\nroundr(rate) = round(Ts, Float64(Ts(rate)), digits=2)\nplot!(pf1, tf .- te1, -r1 .* (tf .- te1),\n label = \"Linear Fit: R1 = $(roundr(r1))\")\n\nMaybe that poor fit was just one unlucky trial? Repeat the log-linear fit many times.\n\nysim_log = log.(abs.(ysim))\nysim_log .-= ysim_log[1,:]'\nr1sim = vec(A1pinv * ysim_log)\nr1sim = Ts.(r1sim)\n\nph1 = histogram(r1sim, bins=16:0.2:32,\n label = \"Mean=$(roundr(mean(r1sim))), σ=$(roundr(std(r1sim)))\",\n xaxis = (\"R1 estimate via log-linear fit\", (16, 32)./s, (16:2:32)./s),\n)\nplot!(r_true*[1,1], [0, 140])","category":"section"},{"location":"generated/mri/95-varpro1/#CRB","page":"Variable Projection: One exponential","title":"CRB","text":"Compute CRB for precision of unbiased estimator. This requires inverting the Fisher information matrix. Here the Fisher information matrix has units, so Julia's built-in inverse inv does not work. See 2.4.5.2 of Fessler 2024 book for tips.\n\n\"\"\"\nMatrix inverse for matrix whose units are suitable for inversion,\nmeaning `X = Diag(left) * Z * Diag(right)`\nwhere `Z` is unitless and `left` and `right` are vectors with units.\n(Fisher information matrices always have this structure.)\n\"\"\"\nfunction inv_unitful(X::Matrix{<:Number})\n    right = oneunit.(X[1,:]) # units for \"right side\" of matrix\n    left = oneunit.(X[:,1] / right[1]) # units for \"left side\" of matrix\n    N = size(X,1)\n    Z = [X[i,j] / (left[i] * right[j]) for i in 1:N, j in 1:N]\n    Zinv = inv(Z) # Z should be unitless if X has inverse-appropriate units\n    Xinv = [Zinv[i,j] / (right[i] * left[j]) for i in 1:N, j in 1:N]\n    return Xinv\nend;\nnothing #hide\n\nGradients of y_true = x_true * exp.(- r_true * tm)\n\ngrad1 = @. exp(-r_true * tm) # x\ngrad2 = @. -x_true * tm * exp(-r_true * tm) # r\ngrad = [grad1 grad2]\nfish1 = grad' * grad / σ^2\n\nCompute CRB from Fisher information via unitful matrix inverse\n\ncrb = inv_unitful(fish1)\ncrb_r1 = Ts(sqrt(crb[2,2]))\ncrb_r1_xknown = Ts(sqrt(1/fish1[2,2]))\n\nplot!(annotation = (27, 100,\n \"CRB = $(roundr(crb_r1)) or $(roundr(crb_r1_xknown))\", :red))","category":"section"},{"location":"generated/mri/95-varpro1/#Log-linear-fit-to-early-echoes","page":"Variable Projection: One exponential","title":"Log-linear fit to early echoes","text":"Try discarding some of the later echoes that have worse SNR. The results depend a lot on how many echoes are used. In this case, K = 4 or 5 works well, but how would you know in practice?\n\nK = 3 # keep first few echos\nA1K = A1[1:K,:]\nA1pinvK = inv(A1K'*A1K) * A1K'\nr1simK = vec(A1pinvK * ysim_log[1:K,:])\nr1simK = Ts.(r1simK)\n\nphK = histogram(r1simK, bins=16:0.2:32,\n label = \"Mean=$(roundr(mean(r1simK))), σ=$(roundr(std(r1simK)))\",\n xaxis = (\"R1 estimate via log-linear fit K=$K\", (16, 32)./s, (16:2:32)./s),\n)\nplot!(r_true*[1,1], [0, 140])\nplot!(annotation = (27, 100,\n \"CRB = $(roundr(crb_r1)) or $(roundr(crb_r1_xknown))\", :red))","category":"section"},{"location":"generated/mri/95-varpro1/#Compare-to-dictionary-matching","page":"Variable Projection: One exponential","title":"Compare to dictionary matching","text":"This approach is essentially a quantized maximum-likelihood estimator. Here the quantization interval of 0.1/s turns out to be much smaller than the estimator standard deviation of 0.6/s, so the quantization error seems negligible.\n\nIt is essential to normalize the dictionary atoms by the Euclidean norm. The reason why is exactly the same as the VarPro derivation.\n\nr_list = range(0/s, 40/s, 401) # linear spacing?\ndict = signal(1, r_list')\ndict_norm = dict ./ norm.(eachcol(dict))'\npd = plot(\n  plot(tm, dict[:,1:10:end], title=\"Dictionary\"),\n  plot(tm, dict_norm[:,1:10:end], title=\"Normalized Dictionary\"),\n)\n\nInner products with (normalized) dictionary atoms\n\ntmp = dict_norm' * real(ysim)\ntmp = argmax.(eachcol(tmp)) # find max\nr_dm = r_list[tmp]; # R2 value from dictionary\n\nph_dm = histogram(r_dm, bins=16:0.2:32,\n label = \"Mean=$(roundr(mean(r_dm))), σ=$(roundr(std(r_dm)))\",\n xaxis = (\"R1 estimate via dictionary matching\", (16, 32)./s, (16:2:32)./s),\n)\nplot!(r_true*[1,1], [0, 140])\nplot!(annotation = (27, 100,\n \"CRB = $(roundr(crb_r1)) or $(roundr(crb_r1_xknown))\", :red))","category":"section"},{"location":"generated/mri/95-varpro1/#WLS-for-log-linear-fit","page":"Variable Projection: One exponential","title":"WLS for log-linear fit","text":"Compare to WLS via error propagation Much faster than dictionary matching!\n\nDerive weights. The log of the normalized signal value (ignoring phase correction and the absolute value) is approximately (using a 1st-order Taylor expansion)\n\nv_m\n= log(y_m  y_1)\n= log(y_m) - log(y_1)\n log(bary_m) - log(bary_1)\n+ (1bary_m) (y_m - bary_m)\n- (1bary_1) (y_1 - bary_1)\n\nso the variance of that log value is\n\nmathrmVar(v_m)\n (1bary_m^2)  mathrmVar(y_m - bary_m)\n+ (1bary_1^2)  mathrmVar(y_1 - bary_1)\n= σ^2  ( 1bary_m^2 + 1bary_1^2 ) \n\nWe would like to perform the WLS fit using the reciprocal of that variance. However, the expected values bary_m = mathbbE(y_m) are unknown because they depend on the latent parameter(s).\n\nIn practice we use a \"plug-in\" estimate using the observed data in place of the expectation:\n\nw_m = 1  ( 1y_m^2 + 1y_1^2 ) \n\nThe noise variance σ^2 is the same for all m so it is irrelevant to the WLS fit:\n\nargmin_x (A x - b) W (A x - b)\n\nwhere here b is the log normalized data b_m = log(y_m  y_1) and x here denotes the rate parameter r.\n\nFor m=1 we have v_1 = 1 by construction, which has zero variance. Our log-linear model -r Δt is explicitly 0 at Δt_1 = 0, i.e., A_11 = 0, so that first (normalized) data point provides no information and is inherently excluded from the linear fit (for any weighting).\n\n\"\"\"\n   wls_exp_fit(y)\nFit rate of a single exponential using weighted least-squares (WLS)\nExpects non-log data.\nUses global `A1`.\nReturns scalar rate estimate.\n\"\"\"\nfunction wls_exp_fit(y)\n    w = @. (1 / y^2 + 1 / y[1]^2) # drop irrelevant σ^2\n    w = 1 ./ w # weights via 1st-order Taylor approx\n    A1w = w .* A1\n    ylog = @. log(abs(y ./ y[1]))\n    return only(A1w' * ylog) / only(A1w' * A1)\nend\n\nr1_wls = Ts.(wls_exp_fit.(real(eachcol(ysim))))\n\nph_wls = histogram(r1_wls, bins=16:0.2:32,\n label = \"Mean=$(roundr(mean(r1_wls))), σ=$(roundr(std(r1_wls)))\",\n xaxis = (\"R1 estimate via log-WLS\", (16, 32)./s, (16:2:32)./s),\n)\nplot!(r_true*[1,1], [0, 270])\nplot!(annotation = (27, 100,\n \"CRB = $(roundr(crb_r1)) or $(roundr(crb_r1_xknown))\", :red))","category":"section"},{"location":"generated/mri/95-varpro1/#Phantom-illustration","page":"Variable Projection: One exponential","title":"Phantom illustration","text":"r2_values = [20, 0, -3, -4, 5, 6, 7, 8, 9, -2] / s\nfovs = (256mm, 256mm)\nparams = ellipse_parameters(SheppLoganBrainWeb(); fovs, disjoint=true)\nparams = [(p[1:5]..., r2_values[i]) for (i, p) in enumerate(params)]\nob = ellipse(params)\ny = range(-fovs[2]/2, fovs[2]/2, 256)\nx = range(-205/2, 205/2, 206) * (y[2]-y[1])\noversample = 3\nr2_map_true = phantom(x, y, ob[1:1], 1) + # trick to avoid edge issues\n    phantom(x, y, ob[3:end], oversample)\nmask = r2_map_true .> 0/s\nx_map_true = @. 100 * mask * cis(π/2 + x / 100mm) # non-uniform phase\n\nclimr = (0,30) ./ s\np2r = jim(x, y, r2_map_true, \"R2* Map\";\n clim = climr, xlabel=\"x\", ylabel=\"y\", color=:cividis)\np2 = plot(\n p2r,\n jim(x, y, x_map_true, \"|M0 Map|\"; xlabel=\"x\", ylabel=\"y\"),\n jim(x, y, angle.(x_map_true), \"∠M0 Map\"; xlabel=\"x\", ylabel=\"y\", color=:hsv);\n layout = (1,3), size=(800,300),\n)\n\nSimulate multi-echo data\n\ny_true_phantom = signal.(x_map_true, r2_map_true)\ny_true_phantom = stack(y_true_phantom)\ny_true_phantom = permutedims(y_true_phantom, [2, 3, 1]) # (nx,ny,M)\ndim = size(y_true_phantom)\ny_phantom = y_true_phantom + σ * randn(Tc, dim...)\n\npyp = plot(\n jim(x, y, y_phantom; title=\"|Echo images|\"),\n jim(x, y, angle.(y_phantom); title=\"∠Echo images\", color=:hsv),\n layout = (1,2), size=(800,300),\n)\n\nPhase correct\n\ny_phantom_dephased = @. conj(sign(y_phantom[:,:,1])) * y_phantom\njim(x, y, angle.(y_phantom_dephased); title=\"∠Echo images after dephasing\", color=:hsv)\n\nTake real part\n\ny_phantom_realed = real(y_phantom_dephased)\njim(x, y, y_phantom_realed; title=\"Real(Echo images) after dephasing\")\n\nWLS estimate of R2* from real part\n\nr2_map_wls = Ts.(wls_exp_fit.(eachslice(y_phantom_realed; dims=(1,2))))\nr2_map_wls .*= mask\nRGB255(args...) = RGB((args ./ 255)...)\necolor = cgrad([RGB255(230, 80, 65), :black, RGB255(23, 120, 232)])\nrmse = sqrt(mean(abs2.(r2_map_wls[mask] - r2_map_true[mask])))\nplot(p2r,\n jim(x, y, r2_map_wls, \"R2* Map via WLS\";\n  clim = climr, xlabel=\"x\", ylabel=\"y\", color=:cividis),\n jim(x, y, r2_map_wls - r2_map_true, \"R2* Error \\n RMSE=$(roundr(rmse))\";\n  clim = (-3, 3) ./ s, color=ecolor);\n layout = (1,3), size=(800,300),\n)","category":"section"},{"location":"generated/mri/95-varpro1/#Future-work","page":"Variable Projection: One exponential","title":"Future work","text":"affine fit via LS and WLS and ML","category":"section"},{"location":"generated/mri/95-varpro1/#Reproducibility","page":"Variable Projection: One exponential","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/mri/99-varpro2/#varpro2","page":"VarPro: Two exponentials","title":"VarPro: Two exponentials","text":"Illustrate fitting bi-exponential model to data.\n\nSee:\n\nVarPro blog\nVP4Optim.jl has biexponential fits\nVarpro.jl\n\nThis page comes from a single Julia file: 99-varpro2.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 99-varpro2.ipynb, or open it in binder here: 99-varpro2.ipynb.","category":"section"},{"location":"generated/mri/99-varpro2/#Setup","page":"VarPro: Two exponentials","title":"Setup","text":"Packages needed here.\n\nimport ForwardDiff\nusing ImagePhantoms: ellipse, ellipse_parameters, phantom, SheppLoganBrainWeb\nusing Statistics: mean, std\nusing Plots: default, gui, histogram, plot, plot!, scatter, scatter!\nusing Plots: cgrad, RGB\ndefault(markerstrokecolor=:auto, label=\"\", widen = true)\nusing LaTeXStrings\nusing LinearAlgebra: norm, Diagonal, diag, diagm, qr\nusing MIRTjim: jim\nusing Random: seed!; seed!(0)\n#using Unitful: @u_str, ms, s, mm\nms = 0.001; s = 1; mm = 1 # avoid Unitful due to ForwardDiff\nusing InteractiveUtils: versioninfo","category":"section"},{"location":"generated/mri/99-varpro2/#Double-exponential-(bi-exponential)","page":"VarPro: Two exponentials","title":"Double exponential (bi-exponential)","text":"We explore a simple case: fitting a bi-exponential to some noisy data:\n\ny_m = c_a e^- r_a t_m + c_b e^- r_b t_m + ϵ_m\nquad\nm = 1M\n\nThe four unknown parameters here are:\n\nthe decay rates r_a r_b  0\nthe amplitudes (aka coefficients) c_a c_b (that could be complex in some MRI settings)\n\nTf = Float32\nTc = Complex{Tf}\nM = 18 # how many samples (more than in single exponential demo!)\nΔte = 10ms # echo spacing\nte1 = 5ms # time of first echo\ntm = Tf.(te1 .+ (0:(M-1)) * Δte) # echo times\nLin = (:ca, :cb) # linear model parameter names\nNon = (:ra, :rb) # nonlinear model parameter names\nTlin = NamedTuple{Lin}\nTnon = NamedTuple{Non}\nTall = NamedTuple{(Lin..., Non...)}\nc_true = Tlin(Tf.([60, 40])) # AU\nr_true = Tnon(Tf.([100/s, 20/s]))\nx_true = (; c_true..., r_true...) # all parameters","category":"section"},{"location":"generated/mri/99-varpro2/#Signal-model","page":"VarPro: Two exponentials","title":"Signal model","text":"This next function is the signal basis function(s) from physics. This is the only function that is model-specific.\n\nsignal_bases(ra::Number, rb::Number, t::Number) =\n    [exp(-t * ra); exp(-t * rb)]; # bi-exponential model\nnothing #hide\n\nThe following signal helper functions apply to many models having a mix of linear and nonlinear signal parameters.\n\nHere there is just one scan parameter (echo time).\n\nThese functions would need to be generalized\n\nto handle multiple scan parameters (e.g., echo time, phase cycling factor, flip angle).\n\nThey would also need to be generalized\n\nto handle models with \"known\" parameters (e.g., B0 and B1+).\n\nsignal_bases(non::Tnon, t::Number) =\n    signal_bases(non..., t)\nsignal_bases(non::Tnon, tv::AbstractVector) =\n    stack(t -> signal_bases(non, t), tv; dims=1);\nnothing #hide\n\nSignal model that combines nonlinear and linear effects:\n\nsignal(lin::Tlin, non::Tnon, tv::AbstractVector) =\n    signal_bases(non, tv) * collect(lin);\nnothing #hide\n\nSignal model helpers:\n\nsignal(lin::AbstractVector, non::Tnon, tv::AbstractVector) =\n   signal(Tlin(lin), non, tv)\nsignal(lin, non::AbstractVector, tv::AbstractVector) =\n   signal(lin, Tnon(non), tv)\nfunction signal(x::Tall, tv::AbstractVector)\n   fun = name -> getfield(x, name)\n   signal(Tlin(fun.(Lin)), Tnon(fun.(Non)), tv)\nend\nsignal(x::AbstractVector, tv::AbstractVector) =\n   signal(Tall(x), tv)","category":"section"},{"location":"generated/mri/99-varpro2/#Simulate-data:","page":"VarPro: Two exponentials","title":"Simulate data:","text":"y_true = signal(c_true, r_true, tm)\n@assert y_true == signal(x_true, tm)\ntf = Tf.(range(0, M, 201) * Δte) # fine sampling for plots\nyf = signal(c_true, r_true, tf)\nxaxis_t = (\"t [ms]\", (0,200), (0:4:M)*Δte/ms) # no units\npy = plot( xaxis = xaxis_t, yaxis = (\"y\", (0,100)) )\nplot!(py, tf/ms, yf, color=:black)\nscatter!(py, tm/ms, y_true, label = \"Noiseless data, M=$M samples\")","category":"section"},{"location":"generated/mri/99-varpro2/#Random-phase-and-noise","page":"VarPro: Two exponentials","title":"Random phase and noise","text":"Actual MRI data has some phase and noise.\n\nphase_true = rand() * 2π + 0π\ny_true_phased = Tc.(cis(phase_true) * y_true)\n\nsnr = 25 # dB\nsnr2sigma(db, y) = 10^(-db/20) * norm(y) / sqrt(length(y))\nσ = Tf(snr2sigma(snr, y_true_phased))\nyc = y_true_phased + σ * randn(Tc, M)\n@show 20 * log10(norm(yc) / norm(yc - y_true_phased)) # check σ\n\nThe phase of the noisy data becomes unreliable for low signal values:\n\npp = scatter(tm/ms, angle.(y_true_phased), label = \"True data\",\n xaxis = xaxis_t,\n yaxis = (\"Phase\", (-π, π), ((-1:1)*π, [\"-π\", \"0\", \"π\"])),\n)\nscatter!(tm, angle.(yc), label=\"Noisy data\")\n\npc = scatter(tm/ms, real(yc),\n label = \"Noisy data - real part\",\n xaxis = xaxis_t,\n ylim = (-100, 100),\n)\nscatter!(pc, tm/ms, imag(yc),\n label = \"Noisy data - imag part\",\n)","category":"section"},{"location":"generated/mri/99-varpro2/#Phase-correction","page":"VarPro: Two exponentials","title":"Phase correction","text":"Phase correct signal using phase of first (noisy) data point\n\nyr = conj(sign(yc[1])) .* yc\n\npr = deepcopy(py)\nscatter!(pr, tm/ms, real(yr),\n label = \"Phase corrected data - real part\",\n xaxis = xaxis_t,\n ylim = (-5, 105),\n marker = :square,\n)\nscatter!(pr, tm/ms, imag(yr),\n label = \"Phase corrected data - imag part\",\n)\n\nExamine the distribution of real part after phase correction\n\nfunction make1_phase_corrected_signal()\n    phase_true = rand() * 2π\n    y_true_phased = Tc.(cis(phase_true) * y_true)\n    yc = y_true_phased + σ * randn(Tc, M)\n    yr = conj(sign(yc[1])) .* yc\nend\n\nN = 2000\nysim = stack(_ -> make1_phase_corrected_signal(), 1:N)\ntmp = ysim[end,:];\n\npe = scatter(real(tmp), imag(tmp), aspect_ratio=1,\n xaxis = (\"real(y_$M)\", (-4,8), -3:8),\n yaxis = (\"imag(y_$M)\", (-6,6), -5:5),\n)\nplot!(pe, real(y_true[end]) * [1,1], [-5, 5])\nplot!(pe, [-5, 5] .+ 3, imag(y_true[end]) * [1,1])\n\nHistogram of the real part looks reasonably Gaussian\n\nph = histogram((real(tmp) .- real(y_true[end])) / (σ/√2), bins=-4:0.1:4,\n xlabel = \"Real part of phase-corrected signal y_$M\")\n\n\nroundr(rate) = round(rate, digits=2);\nnothing #hide","category":"section"},{"location":"generated/mri/99-varpro2/#CRB","page":"VarPro: Two exponentials","title":"CRB","text":"Compute CRB for precision of unbiased estimator. This requires inverting the Fisher information matrix. If the Fisher information matrix has units, then Julia's built-in inverse inv does not work. See 2.4.5.2 of Fessler 2024 book for tips.\n\n\"\"\"\nMatrix inverse for matrix whose units are suitable for inversion,\nmeaning `X = Diag(left) * Z * Diag(right)`\nwhere `Z` is unitless and `left` and `right` are vectors with units.\n(Fisher information matrices always have this structure.)\n\n[irrelevant when units are excluded]\n\"\"\"\nfunction inv_unitful(X::Matrix{<:Number})\n    right = oneunit.(X[1,:]) # units for \"right side\" of matrix\n    left = oneunit.(X[:,1] / right[1]) # units for \"left side\" of matrix\n    N = size(X,1)\n    Z = [X[i,j] / (left[i] * right[j]) for i in 1:N, j in 1:N]\n    Zinv = inv(Z) # Z should be unitless if X has inverse-appropriate units\n    Xinv = [Zinv[i,j] / (right[i] * left[j]) for i in 1:N, j in 1:N]\n    return Xinv\nend;\n\n\nsignal(x) = signal(x, tm)\n@assert y_true == signal(collect(x_true))\n\nJacobian of signal w.r.t. both linear and nonlinear parameters\n\njac = ForwardDiff.jacobian(signal, collect(x_true));\nnothing #hide\n\nFisher information\n\nfish = jac' * jac / σ^2;\nnothing #hide\n\nCompute CRB from Fisher information via matrix inverse\n\ncrb = inv_unitful(fish)\n\nround3(x) = round(x; digits=3)\ncrb_std = Tall(round3.(sqrt.(diag(crb)))) # relabel CRB std. deviations","category":"section"},{"location":"generated/mri/99-varpro2/#Dictionary-matching","page":"VarPro: Two exponentials","title":"Dictionary matching","text":"This approach is essentially a quantized maximum-likelihood estimator. Here the quantization interval of 0.5/s turns out to be much smaller than the estimator standard deviation, so the quantization error seems negligible.\n\nSimple dot products seem inapplicable to a 2-pool model, so we use VarPro.\n\nThe VarPro cost function (for complex coefficients) becomes\n\nf(r) = -(Ay) (AA)^-1 Ay\n\nwhere A = A(r) is a M  2 matrix for each r.\n\nBy applying the QR decomposition of A, the cost function simplifies to -Qy₂, which is a natural extension of the dot product used in dictionary matching.\n\nra_list = Tf.(range(50/s, 160/s, 221)) # linear spacing?\nrb_list = Tf.(range(0/s, 40/s, 81)) # linear spacing?\nbases_unnormalized(ra, rb) = signal_bases((;ra, rb), tm)\ndict = bases_unnormalized.(ra_list, rb_list');\nnothing #hide\n\nPlot the fast and slow dictionary components\n\ntmp = stack(first ∘ eachcol, dict[:,1])\npd1 = plot(tm/ms, tmp[:,1:5:end]; xaxis=xaxis_t, marker=:o)\ntmp = stack(last ∘ eachcol, dict[1,:])\npd2 = plot(tm/ms, tmp[:,1:5:end]; xaxis=xaxis_t, marker=:o)\npd12 = plot(pd1, pd2, plot_title = \"Dictionary\")\n\ndict_q = map(A -> Matrix(qr(A).Q), dict)\ndict_q = map(A -> sign(A[1]) * A, dict_q); # preserve sign of 1st basis\nnothing #hide\n\ntmp = stack(first ∘ eachcol, dict_q[:,1])\npq1 = plot(tm/ms, tmp[:,1:5:end]; xaxis=xaxis_t, marker=:o)\ntmp = stack(last ∘ eachcol, dict_q[1,:])\npq2 = plot(tm/ms, tmp[:,1:5:end]; xaxis=xaxis_t, marker=:o)\npq12 = plot(pq1, pq2, plot_title = \"Orthogonalized Dictionary\")\n\nvarpro_cost(Q::Matrix, y::AbstractVector) = -norm(Q'*y)\nvarpro_best(y) = findmin(Q -> varpro_cost(Q, y), dict_q)[2]\n\nif !@isdefined(i_vp) # perform dictionary matching via VarPro\n    i_vp = map(varpro_best, eachcol(ysim));\nend\nra_dm = map(i -> ra_list[i[1]], i_vp) # dictionary matching estimates\nrb_dm = map(i -> rb_list[i[2]], i_vp)\n\nph_ra_dm = histogram(ra_dm, bins=60:5:160,\n label = \"Mean=$(roundr(mean(ra_dm))), σ=$(roundr(std(ra_dm)))\",\n xaxis = (\"Ra estimate via dictionary matching\", (60, 160)./s),\n)\nplot!(r_true.ra*[1,1], [0, 3e2])\nplot!(annotation = (140, 200, \"CRB(Ra) = $(roundr(crb_std.ra))\", :red))\n\nph_rb_dm = histogram(rb_dm, bins=14:0.5:26,\n label = \"Mean=$(roundr(mean(rb_dm))), σ=$(roundr(std(rb_dm)))\",\n xaxis = (\"Rb estimate via dictionary matching\", (14, 26)./s),\n)\nplot!(r_true.rb*[1,1], [0, 3e2])\nplot!(annotation = (24, 200, \"CRB = $(roundr(crb_std.rb))\", :red))","category":"section"},{"location":"generated/mri/99-varpro2/#Future-work","page":"VarPro: Two exponentials","title":"Future work","text":"Compare to ML via VarPro\nCompare to ML via NLLS\nCost contours, before and after eliminating x\nMM approach?\nGD?\nNewton's method?\nUnits?","category":"section"},{"location":"generated/mri/99-varpro2/#Reproducibility","page":"VarPro: Two exponentials","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/mri/6-precon/#6-nufft","page":"DCF-based \"preconditioning\" in MRI","title":"DCF-based \"preconditioning\" in MRI","text":"This example illustrates how \"preconditioning\" based on sampling density compensation factors (DCFs) affects image reconstruction in MRI using the Julia language.\n\nThis page comes from a single Julia file: 6-precon.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 6-precon.ipynb, or open it in binder here: 6-precon.ipynb.\n\nThe bottom line here is that DCF-based preconditioning gives an apparent speed-up when staring from a zero initial image, but leads to increased noise (worse error). Choosing a smart starting image, like an appropriately scaled gridding image, provides comparable speed-up without compromising noise.\n\nFirst we add the Julia packages that are need for these examples. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"InteractiveUtils\"\n        \"ImagePhantoms\"\n        \"LaTeXStrings\"\n        \"MIRTjim\"\n        \"MIRT\"\n        \"Plots\"\n        \"Unitful\"\n    ])\nend\n\nNow tell this Julia session to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing ImagePhantoms: shepp_logan, SheppLoganEmis, spectrum, phantom #, Gauss2\nusing LaTeXStrings\nusing LinearAlgebra: I, norm\nusing MIRTjim: jim, prompt # jiffy image display\nusing MIRT: Anufft, diffl_map, ncg\nusing Plots; default(label=\"\", markerstrokecolor=:auto)\nusing Random: seed!; seed!(0)\nusing Unitful: mm # physical units (mm here)\nusing InteractiveUtils: versioninfo\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && jim(:prompt, true);\nnothing #hide","category":"section"},{"location":"generated/mri/6-precon/#Radial-k-space-sampling","page":"DCF-based \"preconditioning\" in MRI","title":"Radial k-space sampling","text":"We consider radial sampling as a simple representative non-Cartesian case. Consider imaging a 256mm × 256mm field of FOV with the goal of reconstructing a 128 × 128 pixel image. The following radial and angular k-space sampling is reasonable.\n\nN = 128\nFOV = 256mm # physical units!\nΔx = FOV / N # pixel size\nkmax = 1 / 2Δx\nkr = ((-N÷2):(N÷2)) / (N÷2) * kmax # radial sampling in k-space\nNr = length(kr) # N+1\nNϕ = N-2 # theoretically should be about π/2 * N\nkϕ = (0:Nϕ-1)/Nϕ * π # angular samples\nνx = kr * cos.(kϕ)' # N × Nϕ k-space sampling in cycles/mm\nνy = kr * sin.(kϕ)'\npsamp = plot(νx, νy, size = (420,400),\n    xaxis = (L\"\\nu_{\\mathrm{x}}\", (-1,1) .* (1.1 * kmax), (-1:1) * kmax),\n    yaxis = (L\"\\nu_{\\mathrm{y}}\", (-1,1) .* (1.1 * kmax), (-1:1) * kmax),\n    aspect_ratio = 1,\n    title = \"Radial k-space sampling\",\n)\n\nisinteractive() && prompt();\nnothing #hide\n\nFor the NUFFT routines considered here, the sampling arguments must be \"Ω\" values: digital frequencies have pseudo-units of radians / pixel.\n\nΩx = (2π * Δx) * νx # N × Nϕ grid of k-space sample locations\nΩy = (2π * Δx) * νy # in pseudo-units of radians / sample\n\npom = scatter(Ωx, Ωy,\n    xaxis = (L\"\\Omega_x\", (-1,1) .* 1.1π, ((-1:1)*π, [\"-π\", \"0\", \"π\"])),\n    yaxis = (L\"\\Omega_y\", (-1,1) .* 1.1π, ((-1:1)*π, [\"-π\", \"0\", \"π\"])),\n    aspect_ratio = 1, markersize = 0.5,\n    title = \"Radial k-space sampling\",\n)\n\nisinteractive() && prompt();\nnothing #hide","category":"section"},{"location":"generated/mri/6-precon/#Radial-k-space-data-for-Shepp-Logan-phantom","page":"DCF-based \"preconditioning\" in MRI","title":"Radial k-space data for Shepp-Logan phantom","text":"Get the ellipse parameters for a MRI-suitable version of the Shepp-Logan phantom and calculate (analytically) the radial k-space data. Then display in polar coordinates.\n\nobject = shepp_logan(SheppLoganEmis(); fovs=(FOV,FOV))\ndata = spectrum(object).(νx,νy)\ndata = data / oneunit(eltype(data)) # abandon units at this point\ndscale = 10000\njimk = (args...; kwargs...) -> jim(kr, kϕ, args...;\n    xaxis = (L\"k_r\", (-1,1) .* kmax, (-1:1) .* maximum(abs, kr)),\n    yaxis = (L\"k_{\\phi}\", (0,π), (0,π)),\n    aspect_ratio = :none,\n    kwargs...\n)\npk = jimk(abs.(data) / dscale; title=\"k-space data magnitude / $dscale\")\n\nHere is what the phantom should look like ideally.\n\nx = (-(N÷2):(N÷2-1)) * Δx\ny = (-(N÷2):(N÷2-1)) * Δx\nideal = phantom(x, y, object, 2)\nclim = (0, 9)\np0 = jim(x, y, ideal; xlabel=L\"x\", ylabel=L\"y\", clim, size = (460,400),\n title=\"True Shepp-Logan phantom\")\n\nisinteractive() && prompt();\nnothing #hide","category":"section"},{"location":"generated/mri/6-precon/#NUFFT-operator","page":"DCF-based \"preconditioning\" in MRI","title":"NUFFT operator","text":"with sanity check\n\nA = Anufft([vec(Ωx) vec(Ωy)], (N,N); n_shift = [N/2,N/2]) # todo: odim=(Nr,Nϕ)\ndx = FOV / N # pixel size\ndx = dx / oneunit(dx) # abandon units for now\nAx_to_y = Ax -> dx^2 * reshape(Ax, Nr, Nϕ) # trick\npj1 = jimk(abs.(Ax_to_y(A * ideal)) / dscale, \"|A*x|/$dscale\")\npj2 = jimk(abs.(Ax_to_y(A * ideal) - data) / dscale, \"|A*x-y|/$dscale\")\nplot(pk, pj1, pj2)\n\nisinteractive() && prompt();\nnothing #hide","category":"section"},{"location":"generated/mri/6-precon/#Density-compensation","page":"DCF-based \"preconditioning\" in MRI","title":"Density compensation","text":"Radial sampling needs (N+1) DSF weights along k-space polar coordinate.\n\nWe use the improved method of Lauzon&Rutt, 1996 and Joseph, 1998.\n\ndν = 1/FOV # k-space radial sample spacing\ndcf = π / Nϕ * dν * abs.(kr) # see lauzon:96:eop, joseph:98:sei\ndcf[kr .== 0/mm] .= π * (dν/2)^2 / Nϕ # area of center disk\ndcf = dcf / oneunit(eltype(dcf)) # kludge: units not working with LinearMap now\ngridded4 = A' * vec(dcf .* data)\np4 = jim(x, y, gridded4; xlabel=L\"x\", ylabel=L\"y\", clim,\n size = (460,400),\n title=\"NUFFT gridding with better ramp-filter DCF\")\n\nisinteractive() && prompt();\nnothing #hide\n\nA profile shows it is \"decent\" but not amazing.\n\npp = plot(x, real(gridded4[:,N÷2]), label=\"Modified ramp DCF\")\nplot!(x, real(ideal[:,N÷2]), label=\"Ideal\", xlabel=L\"x\", ylabel=\"middle profile\")\n\nisinteractive() && prompt();\nnothing #hide","category":"section"},{"location":"generated/mri/6-precon/#Unregularized-iterative-MRI-reconstruction","page":"DCF-based \"preconditioning\" in MRI","title":"Unregularized iterative MRI reconstruction","text":"Apply a few iterations of conjugate gradient (CG) to approach a minimizer of the least-squares cost function:\n\narg min_x frac12  A x - y ₂²\n\nUsing a zero-image as the starting point x₀ leads to slow convergence.\n\nUsing \"preconditioning\" by including DCF values in the cost function (a kind of weighted LS) appears to give much faster convergence, but leads to suboptimal image quality especially in the presence of noise.\n\nUsing a decent initial image (e.g., a gridded image with appropriate scaling) is preferable.\n\nThere is a subtle point here about dx when converting the Fourier integral to a sum. Here yideal is data/dx^2.\n\nnrmse = x -> norm(x - ideal) / norm(ideal) * 100\nfun = (x, iter) -> nrmse(x)\n\nsnr2sigma(db, y) = 10^(-db/20) * norm(y) / sqrt(length(y))\nyideal = vec(data/dx^2)\nσnoise = snr2sigma(40, yideal)\nydata = yideal + σnoise * randn(eltype(yideal), size(yideal))\nsnr = 20 * log10(norm(yideal) / norm(ydata - yideal)) # check\ngradf = u -> u - ydata # gradient of f(u) = 1/2 ‖ u - y ‖²\ncurvf = u -> 1 # curvature of f(u)\n\nx0 = 0 * ideal\nniter = 15\nxls0, out0 = ncg([A], [gradf], [curvf], x0; niter, fun)\n\ndefault(linewidth = 2)\nppr = plot(\n xaxis = (\"iteration\", (0, niter)),\n yaxis = (\"NRMSE\", (0, 100), 0:20:100),\n widen = true,\n)\nplot!(ppr, 0:niter, out0,\n label = \"LS-CG ('non-preconditioned'), 0 init\",\n color = :black, marker = :circle,\n);\nnothing #hide\n\n\"Preconditioned\" (aka \"weighted\") version\n\nprecon = vec(repeat(dcf, Nϕ));\nnothing #hide\n\ngradient of fp(u) = 1/2 ‖ P^{1/2} (u - y) ‖²\n\ngradfp = u -> precon .* (u - ydata)\ncurvfp = u -> precon # curvature of fp(u)\n\nxlsp, out2 = ncg([A], [gradfp], [curvfp], x0; niter, fun)\nplot!(ppr, 0:niter, out2,\n label = \"WLS-CG with DCF weighting, 0 init\", color = :red, marker = :x,\n)\n\nSmart start with gridded image\n\nx0 = gridded4 # initial guess: decent gridding reconstruction\nxls1, out1 = ncg([A], [gradf], [curvf], x0; niter, fun)\nplot!(ppr, 0:niter, out1,\n  label = \"LS-CG, gridding init\", color=:blue, marker=:+,\n)\n\nisinteractive() && prompt();\nnothing #hide\n\nThe images look very similar at 15 iterations\n\nelim = (0, 1)\ntmp = stack((ideal, xls0, xlsp, xls1))\nerr = stack((ideal, xls0, xlsp, xls1)) .- ideal\np5 = plot(\n jim(x, y, tmp; title = \"Ideal | 0 init | precon | grid init\",\n  clim, nrow=1, size = (600, 200)),\n jim(x, y, err; title = \"Error\", color=:cividis,\n  clim = elim, nrow=1, size = (600, 200)),\n layout = (2, 1),\n size = (650, 450),\n)\n\nisinteractive() && prompt();\nnothing #hide","category":"section"},{"location":"generated/mri/6-precon/#Reproducibility","page":"DCF-based \"preconditioning\" in MRI","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/mri/3-2d-t/#3-2d-t","page":"2D dynamic: temporal TV","title":"2D dynamic: temporal TV","text":"This example illustrates 2D dynamic MRI image reconstruction from golden angle (GA) radial sampled k-space data collected with multiple coils (parallel MRI), with temporal \"TV\" regularizer (corner-rounded) using the Julia language.\n\nThis page comes from a single Julia file: 3-2d-t.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 3-2d-t.ipynb, or open it in binder here: 3-2d-t.ipynb.\n\nusing ImageGeoms: ImageGeom\nusing ImagePhantoms: shepp_logan, SouthPark, phantom, ellipse, spectrum\nusing MIRTjim: jim, prompt\nusing MIRT: Anufft, diffl_map, ncg\nusing MIRT: ir_mri_sensemap_sim, ir_mri_kspace_ga_radial\nusing Plots: gui, plot, scatter, default; default(markerstrokecolor=:auto, label=\"\")\nusing Plots: gif, @animate, Plots\nusing LinearAlgebra: norm, dot, Diagonal\nusing LinearMapsAA: LinearMapAA, block_diag\nusing Random: seed!\njim(:abswarn, false); # suppress warnings about display of |complex| images\nnothing #hide\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && jim(:prompt, true);\nnothing #hide","category":"section"},{"location":"generated/mri/3-2d-t/#Define-dynamic-image-sequence","page":"2D dynamic: temporal TV","title":"Define dynamic image sequence","text":"N = (60,64)\nfov = 220\nnt = 8 # frames\nig = ImageGeom(; dims = N, deltas=(fov,fov) ./ N)\n\nobject0 = shepp_logan(SouthPark(); fovs = (fov,fov))\nobjects = Array{typeof(object0)}(undef, nt)\nxtrue = Array{ComplexF32}(undef, N..., nt)\nfor it in 1:nt\n    tmp = copy(object0)\n    width2 = 15 + 5 * sin(2π*it/nt) # mouth open/close\n    mouth = tmp[2]\n    tmp[2] = ellipse(mouth.center, (mouth.width[1], width2), mouth.angle[1], mouth.value)\n    objects[it] = tmp\n    xtrue[:,:,it] = phantom(axes(ig)..., tmp, 4)\nend\njimxy = (args...; aspect_ratio=1, kwargs...) ->\n    jim(axes(ig)..., args...; aspect_ratio, kwargs...)\njimxy(xtrue, \"True images\"; nrow=2)\n\nAnimate true image:\n\nanim1 = @animate for it in 1:nt\n    jimxy(xtrue[:,:,it], title=\"Frame $it\")\nend\ngif(anim1; fps = 6)\n\nPlot one time course to see temporal change:\n\nix,iy = 30,14\nplot(1:nt, abs.(xtrue[ix,iy,:]), label=\"ix=$ix, iy=$iy\",\n    marker=:o, xlabel=\"frame\")\n\nisinteractive() && prompt();\nnothing #hide","category":"section"},{"location":"generated/mri/3-2d-t/#Define-k-space-sampling","page":"2D dynamic: temporal TV","title":"Define k-space sampling","text":"accelerate = 3\nnspf = round(Int, maximum(N)/accelerate) # spokes per frame\n\nNro = maximum(N)\nNspoke = nspf * nt\nkspace = ir_mri_kspace_ga_radial(; Nro, Nspoke)\nfovs = (fov, fov)\nkspace[:,:,1] ./= fovs[1]\nkspace[:,:,2] ./= fovs[2]\nkspace = reshape(kspace, Nro, nspf, nt, 2)\n(size(kspace), extrema(kspace))\n\nPlot sampling (in units of cycles/pixel):\n\nps = Array{Any}(undef, nt)\nfor it in 1:nt\n    ps[it] = scatter(kspace[:,:,it,1] * fovs[1], kspace[:,:,it,2] * fovs[2],\n        xtick=(-1:1)*0.5, ytick=(-1:1)*0.5, xlim=(-1,1).*0.5, ylim=(-1,1).*0.5,\n        aspect_ratio=1, markersize=1, title=\"Frame $it\", widen=true)\nend\nplot(ps...; layout=(2,4))\n\nisinteractive() && prompt();\n\nanim2 = @animate for it in 1:nt\n    plot(ps[it])\nend\ngif(anim2; fps = 6)\n\nMake sensitivity maps, normalized so SSoS = 1:\n\nncoil = 2\nsmap_raw = ir_mri_sensemap_sim(dims=N, ncoil=ncoil, orbit_start=[90])\np1 = jim(smap_raw, \"Sensitivity maps raw\");\n\nsum_last = (f, x) -> selectdim(sum(f, x; dims=ndims(x)), ndims(x), 1)\nssos_fun = smap -> sqrt.(sum_last(abs2, smap)) # SSoS\nssos_raw = ssos_fun(smap_raw)\np2 = jim(ssos_raw, \"SSoS for ncoil=$ncoil\");\n\nsmap = smap_raw ./ ssos_raw\np3 = jim(smap, \"Sensitivity maps\");\n\nssos = ssos_fun(smap) # SSoS\n@assert all(≈(1), ssos)\njim(p1, p2, p3)","category":"section"},{"location":"generated/mri/3-2d-t/#System-matrix","page":"2D dynamic: temporal TV","title":"System matrix","text":"Make system matrix for dynamic non-Cartesian parallel MRI:\n\nFs = Array{Any}(undef, nt)\nfor it in 1:nt # a NUFFT object for each frame\n    Ω = [kspace[:,:,it,1][:] kspace[:,:,it,2][:]] * fov * 2pi\n    Fs[it] = Anufft(Ω, N, n_shift = [N...]/2)\nend\n\nBlock diagonal system matrix, with one NUFFT per frame\n\nS = [Diagonal(vec(selectdim(smap, ndims(smap), ic))) for ic in 1:ncoil]\nSO = s -> LinearMapAA(s ; idim=N, odim=N) # LinearMapAO for coil maps\nAS1 = F -> vcat([F * SO(s) for s in S]...); # [A1*S1; ... ; A1*Sncoil]\nnothing #hide\n\nLinear operator input is (N..., nt), output is (nspf*Nro, Ncoil, nt)\n\nA = block_diag([AS1(F) for F in Fs]...) # todo: refine show()\n(size(A), A._odim, A._idim)\n\nSimulate k-space data via an inverse crime (todo):\n\nytrue = A * xtrue\nsnr2sigma = (db, yb) -> # compute noise sigma from SNR (no sqrt(2) needed)\n    10^(-db/20) * norm(yb) / sqrt(length(yb))\nsig = Float32(snr2sigma(50, ytrue))\nseed!(0)\ny = ytrue + sig * randn(ComplexF32, size(ytrue))\nysnr = 20*log10(norm(ytrue) / norm(y - ytrue)) # verify SNR","category":"section"},{"location":"generated/mri/3-2d-t/#Initial-image","page":"2D dynamic: temporal TV","title":"Initial image","text":"Initial image via zero-fill and scaling: todo: should use density compensation, perhaps via VoronoiDelaunay.jl\n\nx0 = A' * y # zero-filled recon (for each frame)\ntmp = A * x0 # (Nkspace, Ncoil, Nframe)\nx0 = (dot(tmp,y) / norm(tmp)^2) * x0 # scale sensibly\njimxy(x0, \"initial image\"; nrow=2)\n\nTemporal finite differences:\n\nDt = diffl_map((N..., nt), length(N)+1 ; T=eltype(A))\ntmp = Dt' * (Dt * xtrue)\njimxy(tmp, \"Time differences are sparse\"; nrow=2)","category":"section"},{"location":"generated/mri/3-2d-t/#Regularized-CG-reconstruction","page":"2D dynamic: temporal TV","title":"Regularized CG reconstruction","text":"Run nonlinear CG on \"temporal edge-preserving\" regularized LS cost function\n\nniter = 90\ndelta = Float32(0.1) # small relative to temporal differences\nreg = Float32(2^20) # trial and error here\nffair = (t,d) -> d^2 * (abs(t)/d - log(1 + abs(t)/d))\npot = z -> ffair(z, delta)\ndpot = z -> z / (Float32(1) + abs(z/delta))\ncost = x -> 0.5 * norm(A*x - y)^2 + reg * sum(pot.(Dt * x))\nfun = (x,iter) -> cost(x)\ngradf = [v -> v - y, u -> reg * dpot.(u)]\ncurvf = [v -> Float32(1), u -> reg]\n(xh, out) = ncg([A, Dt], gradf, curvf, x0 ; niter, fun)\ncosts = [out[i+1][1] for i=0:niter];\nnothing #hide\n\nShow results\n\npr = plot(\n    jimxy(xtrue, \"|xtrue|\"; nrow=2),\n    jimxy(xh, \"|recon|\"; nrow=2),\n    jimxy(xh-xtrue, \"|error|\"; nrow=2),\n    scatter(0:niter, log.(costs), label=\"cost\", xlabel=\"iteration\"),\n)\n\nisinteractive() && prompt();\nnothing #hide\n\nAnimate true, recon, error\n\nanim3 = @animate for it in 1:nt\n    plot(\n        jimxy(xtrue[:,:,it], clim=(0,120), title=\"True\"),\n        jimxy(xh[:,:,it], clim=(0,120), title=\"|Recon|\"),\n        jimxy(xh[:,:,it] - xtrue[:,:,it], clim=(0,30), title=\"|Error|\"),\n        plot_title = \"Frame $it\",\n        layout = (1,3),\n    )\nend\ngif(anim3; fps = 6)","category":"section"},{"location":"generated/mri/3-2d-t/#Reproducibility","page":"2D dynamic: temporal TV","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/mri/2-cs-wl-l1-2d/#2-cs-wl-l1-2d","page":"Compressed Sensing 2D","title":"Compressed Sensing 2D","text":"This example illustrates how to perform 2D compressed sensing image reconstruction from Cartesian sampled MRI data with 1-norm regularization of orthogonal wavelet coefficients, using the Julia language.\n\nThis page comes from a single Julia file: 2-cs-wl-l1-2d.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 2-cs-wl-l1-2d.ipynb, or open it in binder here: 2-cs-wl-l1-2d.ipynb.\n\nThis demo is somewhat similar to Fig. 3 in the survey paper \"Optimization methods for MR image reconstruction,\" in Jan 2020 IEEE Signal Processing Magazine, except that the sampling is 1D phase encoding instead of 2D.\n\nPackages used in this demo (run Pkg.add as needed):\n\nusing ImagePhantoms: shepp_logan, SheppLoganEmis, spectrum, phantom\nusing MIRT: embed, Afft, Aodwt\nusing MIRTjim: jim, prompt\nusing MIRT: pogm_restart\nusing LinearAlgebra: norm\nusing Plots; default(markerstrokecolor=:auto, label=\"\")\nusing FFTW: fft\nusing Random: seed!\nusing InteractiveUtils: versioninfo\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && jim(:prompt, true);\nnothing #hide","category":"section"},{"location":"generated/mri/2-cs-wl-l1-2d/#Create-(synthetic)-data","page":"Compressed Sensing 2D","title":"Create (synthetic) data","text":"Shepp-Logan phantom (unrealistic because real-valued):\n\nnx,ny = 192,256\nobject = shepp_logan(SheppLoganEmis(); fovs=(ny,ny))\nXtrue = phantom(-(nx÷2):(nx÷2-1), -(ny÷2):(ny÷2-1), object, 2)\nXtrue = reverse(Xtrue, dims=2)\nclim = (0,9)\njim(Xtrue, \"true image\"; clim)\n\nSomewhat random 1D phase-encode sampling:\n\nseed!(0); sampfrac = 0.2; samp = rand(ny) .< sampfrac; sig = 1\nmod2 = (N) -> mod.((0:N-1) .+ Int(N/2), N) .- Int(N/2)\nsamp .|= (abs.(mod2(ny)) .< Int(ny/8)) # fully sampled center rows\nsamp = trues(nx) * samp'\njim(samp, fft0=true, title=\"k-space sampling ($(100count(samp)/(nx*ny))%)\")\n\nGenerate noisy, under-sampled k-space data (inverse-crime simulation):\n\nytrue = fft(Xtrue)[samp]\ny = ytrue + sig * √(2) * randn(ComplexF32, size(ytrue)) # complex noise!\ny = ComplexF32.(y) # save memory\nysnr = 20 * log10(norm(ytrue) / norm(y-ytrue)) # data SNR in dB\n\nDisplay zero-filled data:\n\nlogger = (x; min=-6) -> log10.(max.(abs.(x) / maximum(abs, x), (10.)^min))\njim(:abswarn, false) # suppress warnings about showing magnitude\njim(logger(embed(ytrue,samp)), fft0=true, title=\"k-space |data| (zero-filled)\",\n    xlabel=\"kx\", ylabel=\"ky\")","category":"section"},{"location":"generated/mri/2-cs-wl-l1-2d/#Prepare-to-reconstruct","page":"Compressed Sensing 2D","title":"Prepare to reconstruct","text":"Creating a system matrix (encoding matrix) and an initial image The system matrix is a LinearMapAA object, akin to a fatrix in Matlab MIRT.\n\nSystem model (\"encoding matrix\") from MIRT:\n\nF = Afft(samp) # operator!\n\nInitial image based on zero-filled reconstruction:\n\nnrmse = (x) -> round(norm(x - Xtrue) / norm(Xtrue) * 100, digits=1)\nX0 = 1.0f0/(nx*ny) * (F' * y)\njim(X0, \"|X0|: initial image; NRMSE $(nrmse(X0))%\")","category":"section"},{"location":"generated/mri/2-cs-wl-l1-2d/#Wavelet-sparsity-in-synthesis-form","page":"Compressed Sensing 2D","title":"Wavelet sparsity in synthesis form","text":"The image reconstruction optimization problem here is\n\narg min_x\nfrac12  A x - y _2^2 + beta   W x _1\n\nwhere y is the k-space data, A is the system model (simply Fourier encoding F here), W is an orthogonal discrete (Haar) wavelet transform, again implemented as a LinearMapAA object. Because W is unitary, we make the change of variables z = W x and solve for z and then let x = W z at the end. In fact we use a weighted 1-norm where only the detail wavelet coefficients are regularized, not the approximation coefficients.\n\nOrthogonal discrete wavelet transform operator (LinearMapAO):\n\nW, scales, _ = Aodwt((nx,ny) ; T = eltype(F))\nisdetail = scales .> 0\njim(\n    jim(scales, \"wavelet scales\"),\n    jim(real(W * Xtrue) .* isdetail, \"wavelet detail coefficients\"),\n)\n\nInputs needed for proximal gradient methods:\n\nAz = F * W' # another operator!\nFnullz = (z) -> 0 # cost function in `z` not needed\nf_gradz = (z) -> Az' * (Az * z - y)\nf_Lz = nx*ny # Lipschitz constant for single coil Cartesian DFT\nregz = 0.03 * nx * ny # oracle from Xtrue wavelet coefficients!\ncostz = (z) -> 1/2 * norm(Az * z - y)^2 + regz * norm(z,1) # 1-norm regularizer\nsoft = (z,c) -> sign(z) * max(abs(z) - c, 0) # soft thresholding\ng_prox = (z,c) -> soft.(z, isdetail .* (regz * c)) # proximal operator (shrink details only)\nz0 = W * X0\njim(z0, \"Initial wavelet coefficients\")","category":"section"},{"location":"generated/mri/2-cs-wl-l1-2d/#Iterate","page":"Compressed Sensing 2D","title":"Iterate","text":"Run ISTA=PGM and FISTA=FPGM and POGM, the latter two with adaptive restart See Kim & Fessler, 2018 for adaptive restart algorithm details.\n\nFunctions for tracking progress:\n\nfunction fun_ista(iter, xk_z, yk, is_restart)\n    xh = W' * xk_z\n    return (costz(xk_z), nrmse(xh), is_restart) # , psnr(xh)) # time()\nend\n\nfunction fun_fista(iter, xk, yk_z, is_restart)\n    xh = W' * yk_z\n    return (costz(yk_z), nrmse(xh), is_restart) # , psnr(xh)) # time()\nend;\nnothing #hide\n\nRun and compare three proximal gradient methods:\n\nniter = 50\nz_ista, out_ista = pogm_restart(z0, Fnullz, f_gradz, f_Lz; mom=:pgm, niter=niter,\n    restart=:none, restart_cutoff=0., g_prox=g_prox, fun=fun_ista)\nXista = W'*z_ista\n@show nrmse(Xista)\n\nz_fista, out_fista = pogm_restart(z0, Fnullz, f_gradz, f_Lz; mom=:fpgm, niter=niter,\n    restart=:gr, restart_cutoff=0., g_prox=g_prox, fun=fun_fista)\nXfista = W'*z_fista\n@show nrmse(Xfista)\n\nz_pogm, out_pogm = pogm_restart(z0, Fnullz, f_gradz, f_Lz; mom=:pogm, niter=niter,\n    restart=:gr, restart_cutoff=0., g_prox=g_prox, fun=fun_fista)\nXpogm = W'*z_pogm\n@show nrmse(Xpogm)\n\njim(\n    jim(Xfista, \"FISTA/FPGM\"),\n    jim(Xpogm, \"POGM with ODWT\"),\n)","category":"section"},{"location":"generated/mri/2-cs-wl-l1-2d/#POGM-is-fastest","page":"Compressed Sensing 2D","title":"POGM is fastest","text":"Plot cost function vs iteration:\n\ncost_ista = [out_ista[k][1] for k=1:niter+1]\ncost_fista = [out_fista[k][1] for k=1:niter+1]\ncost_pogm = [out_pogm[k][1] for k=1:niter+1]\ncost_min = min(minimum(cost_ista), minimum(cost_pogm))\nplot(xlabel=\"iteration k\", ylabel=\"Relative cost\")\nscatter!(0:niter, cost_ista  .- cost_min, label=\"Cost ISTA\")\nscatter!(0:niter, cost_fista .- cost_min, markershape=:square, label=\"Cost FISTA\")\nscatter!(0:niter, cost_pogm  .- cost_min, markershape=:utriangle, label=\"Cost POGM\")\n\nisinteractive() && prompt();\nnothing #hide\n\nPlot NRMSE vs iteration:\n\nnrmse_ista = [out_ista[k][2] for k=1:niter+1]\nnrmse_fista = [out_fista[k][2] for k=1:niter+1]\nnrmse_pogm = [out_pogm[k][2] for k=1:niter+1]\npn = plot(xlabel=\"iteration k\", ylabel=\"NRMSE %\", ylims=(3,6.5))\nscatter!(0:niter, nrmse_ista, label=\"NRMSE ISTA\")\nscatter!(0:niter, nrmse_fista, markershape=:square, label=\"NRMSE FISTA\")\nscatter!(0:niter, nrmse_pogm, markershape=:utriangle, label=\"NRMSE POGM\")\n\nShow error images:\n\np1 = jim(Xtrue, \"true\")\np2 = jim(X0, \"X0: initial\")\np3 = jim(Xpogm, \"POGM recon\")\np5 = jim(X0 - Xtrue, \"X0 error\", clim=(0,2))\np6 = jim(Xpogm - Xtrue, \"Xpogm error\", clim=(0,2))\npe = jim(p2, p3, p5, p6)","category":"section"},{"location":"generated/mri/2-cs-wl-l1-2d/#Reproducibility","page":"Compressed Sensing 2D","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/ct/1-fbp/#1-fbp","page":"FBP Overview","title":"FBP Overview","text":"This example illustrates how to perform filtered back-projection (FBP) image reconstruction in CT using the Julia language.\n\nThis is under construction. See the demos in the Sinograms.jl package instead.\n\nThis page comes from a single Julia file: 1-fbp.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 1-fbp.ipynb, or open it in binder here: 1-fbp.ipynb.\n\nFirst we add the Julia packages that are need for these examples. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"ImagePhantoms\"\n        \"Unitful\"\n        \"Plots\"\n        \"LaTeXStrings\"\n        \"MIRTjim\"\n        \"MIRT\"\n        \"Sinograms\"\n        \"InteractiveUtils\"\n    ])\nend\n\nNow tell this Julia session to use the following packages for this example. Run Pkg.add() in the preceding code block first, if needed.\n\nusing ImagePhantoms: shepplogan, SheppLogan, radon, phantom using Unitful: mm using Plots; default(label=\"\", markerstrokecolor=:auto) using LaTeXStrings using MIRT: difflmap, ncg using Sinograms: todo\n\nusing MIRTjim: jim, prompt\nusing InteractiveUtils: versioninfo\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && jim(:prompt, true);\nnothing #hide\n\nisinteractive() && prompt();\nnothing #hide\n\nGet the ellipse parameters for a CT-suitable version of the Shepp-Logan phantom and calculate (analytically) its sinogram.\n\nif false object = shepplogan(SheppLogan(); fovs=(FOV,FOV)) sino = radon(object).(r,ϕ') data = data / oneunit(eltype(data)) # abandon units at this point jim(kr, kϕ, abs.(data), title=\"k-space data magnitude\",     xlabel=L\"kr\",     ylabel=L\"k{\\phi}\",     xticks = (-1:1) .* maximum(abs, kr),     yticks = (0,π),     ylims = (0,π),     aspectratio = :none, ) end","category":"section"},{"location":"generated/ct/1-fbp/#Reproducibility","page":"FBP Overview","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/mri/1-nufft/#1-nufft","page":"NUFFT Overview","title":"NUFFT Overview","text":"This example illustrates how to use Nonuniform FFT (NUFFT) for image reconstruction in MRI using the Julia language.\n\nThis page comes from a single Julia file: 1-nufft.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 1-nufft.ipynb, or open it in binder here: 1-nufft.ipynb.\n\nSome MRI scans use non-Cartesian sampling patterns (like radial and spiral k-space trajectories, among others), often in the interest of acquisition speed.\n\nImage reconstruction from fully sampled Cartesian k-space data typically uses simple inverse FFT operations, whereas non-Cartesian sampling requires more complicated methods.\n\nThe examples here illustrate both non-iterative (aka \"gridding\") and iterative methods for non-Cartesian MRI reconstruction. For simplicity the examples consider the case of single-coil data and ignore the effects of B0 field inhomogeneity.\n\nFirst we add the Julia packages that are need for these examples. Change false to true in the following code block if you are using any of the following packages for the first time.\n\nif false\n    import Pkg\n    Pkg.add([\n        \"ImagePhantoms\"\n        \"Unitful\"\n        \"Plots\"\n        \"LaTeXStrings\"\n        \"MIRTjim\"\n        \"MIRT\"\n        \"InteractiveUtils\"\n    ])\nend\n\nNow tell this Julia session to use the following packages. Run Pkg.add() in the preceding code block first, if needed.\n\nusing ImagePhantoms: shepp_logan, SheppLoganEmis, spectrum, phantom #, Gauss2\nusing LinearAlgebra: I\nusing Unitful: mm # Allows use of physical units (mm here)\nusing Plots; default(label=\"\", markerstrokecolor=:auto)\nusing LaTeXStrings # for LaTeX in plot labels, e.g., L\"\\alpha_n\"\nusing MIRTjim: jim, prompt # jiffy image display\nusing MIRT: Anufft, diffl_map, ncg\nusing InteractiveUtils: versioninfo\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && jim(:prompt, true);\nnothing #hide","category":"section"},{"location":"generated/mri/1-nufft/#Radial-k-space-sampling","page":"NUFFT Overview","title":"Radial k-space sampling","text":"We focus on radial sampling as a simple representative non-Cartesian case. Consider imaging a 256mm × 256mm field of FOV with the goal of reconstructing a 128 × 128 pixel image. The following radial and angular k-space sampling is reasonable.\n\nN = 128\nFOV = 256mm # physical units!\nΔx = FOV / N # pixel size\nkmax = 1 / 2Δx\nkr = ((-N÷2):(N÷2)) / (N÷2) * kmax # radial sampling in k-space\nNr = length(kr) # N+1\nNϕ = 3N÷2 # theoretically should be about π/2 * N\nkϕ = (0:Nϕ-1)/Nϕ * π # angular samples\nνx = kr * cos.(kϕ)' # N × Nϕ k-space sampling in cycles/mm\nνy = kr * sin.(kϕ)'\nplot(νx, νy,\n    xlabel=L\"\\nu_x\", ylabel=L\"\\nu_y\",\n    aspect_ratio = 1,\n    title = \"Radial k-space sampling\",\n)\n\nisinteractive() && prompt();\nnothing #hide\n\nFor the NUFFT routines considered here, the sampling arguments must be \"Ω\" values: digital frequencies have pseudo-units of radians / pixel.\n\nΩx = (2π * Δx) * νx # N × Nϕ grid of k-space sample locations\nΩy = (2π * Δx) * νy # in pseudo-units of radians / sample\n\nscatter(Ωx, Ωy,\n    xlabel=L\"\\Omega_x\", ylabel=L\"\\Omega_y\",\n    xticks=((-1:1)*π, [\"-π\", \"0\", \"π\"]),\n    yticks=((-1:1)*π, [\"-π\", \"0\", \"π\"]),\n    xlims=(-π,π) .* 1.1,\n    ylims=(-π,π) .* 1.1,\n    aspect_ratio = 1, markersize = 0.5,\n    title = \"Radial k-space sampling\",\n)\n\nisinteractive() && prompt();\nnothing #hide","category":"section"},{"location":"generated/mri/1-nufft/#Radial-k-space-data-for-Shepp-Logan-phantom","page":"NUFFT Overview","title":"Radial k-space data for Shepp-Logan phantom","text":"Get the ellipse parameters for a MRI-suitable version of the Shepp-Logan phantom and calculate (analytically) the radial k-space data. Then display in polar coordinates.\n\nobject = shepp_logan(SheppLoganEmis(); fovs=(FOV,FOV))\n#object = [Gauss2(18mm, 0mm, 100mm, 70mm, 0, 1)] # useful for validating DCF\ndata = spectrum(object).(νx,νy)\ndata = data / oneunit(eltype(data)) # abandon units at this point\ndscale = 10000\njimk = (args...; kwargs...) -> jim(kr, kϕ, args...;\n    xlabel = L\"k_r\",\n    ylabel = L\"k_{\\phi}\",\n    xticks = (-1:1) .* maximum(abs, kr),\n    yticks = (0,π),\n    ylims = (0,π),\n    aspect_ratio = :none,\n    kwargs...\n)\npk = jimk(abs.(data) / dscale; title=\"k-space data magnitude / $dscale\")","category":"section"},{"location":"generated/mri/1-nufft/#Non-iterative-gridding-image-reconstruction","page":"NUFFT Overview","title":"Non-iterative gridding image reconstruction","text":"It would be impossible for a radiologist to diagnose a patient from the k-space data in polar coordinates, so image reconstruction is needed.\n\nThe simplest approach is to (nearest-neighbor) interpolate the k-space data onto a Cartesian grid, and then reconstruction with an inverse FFT.\n\nOne way to do that interpolation is to use a Histogram method in Julia's statistics package.\n\nusing StatsBase: fit, Histogram, weights\n\nThe following function is a work-around because weights in StatsBase is limited to Real data, so here we bin the real and imaginary k-space data separately, and handle the units.\n\nfunction histogram(coord, vals::AbstractArray{<:Number}, edges)\n    u = oneunit(eltype(vals))\n    wr = weights(real(vec(vals / u)))\n    wi = weights(imag(vec(vals / u)))\n    tmp1 = fit(Histogram, coord, wr, edges)\n    tmp2 = fit(Histogram, coord, wi, edges)\n    return u * complex.(tmp1.weights, tmp2.weights)\nend\n\nkx = N * Δx * νx # N × Nϕ k-space sampling in cycles/mm\nky = N * Δx * νy\nbin = (-(N÷2):(N÷2)) .- 0.5 # (N+1,) histogram bin edges\ngridded1 = histogram((vec(kx), vec(ky)), data, (bin,bin))\n\nusing FFTW: ifft, fftshift\ntmp = fftshift(ifft(fftshift(gridded1)))\nx = (-(N÷2):(N÷2-1)) * Δx\ny = (-(N÷2):(N÷2-1)) * Δx\njim(x, y, tmp, title=\"Elementary gridding reconstruction\")\n\nThat crummy gridding method does not work well. Here's what the phantom should look like:\n\nideal = phantom(x, y, object, 2)\nclim = (0, 9)\np0 = jim(x, y, ideal, title=\"True Shepp-Logan phantom\"; clim)","category":"section"},{"location":"generated/mri/1-nufft/#NUFFT-approach-to-gridding","page":"NUFFT Overview","title":"NUFFT approach to gridding","text":"Basic nearest-neighbor gridding does not provide acceptable image quality in MRI, so now we turn to using the NUFFT. For MRI purposes, the NUFFT is a function that maps Cartesian spaced image data into non-Cartesian k-space data. The NFFT.jl package has functions for computing the NUFFT and its adjoint. These are linear mappings (generalizations of matrices), so instead of calling those functions directly, here we use the NUFFT linear map object defined in MIRT that provides a non-Cartesian Fourier encoding \"matrix\".\n\nA = Anufft([vec(Ωx) vec(Ωy)], (N,N); n_shift = [N/2,N/2]) # todo: odim=(Nr,Nϕ)\n\nVerify that the operator A works properly:\n\ndx = FOV / N # pixel size\ndx = dx / oneunit(dx) # abandon units for now\nAx_to_y = Ax -> dx^2 * reshape(Ax, Nr, Nϕ) # trick\npj1 = jimk(abs.(Ax_to_y(A * ideal)) / dscale, \"|A*x|/$dscale\")\npj2 = jimk(abs.(Ax_to_y(A * ideal) - data) / dscale, \"|A*x-y|/$dscale\")\nplot(pj1, pj2)\n\nThis linear map is constructed to map a N × N image into length(Ωx) k-space samples. So its adjoint goes the other direction. However, an adjoint is not an inverse!\n\ngridded2 = A' * vec(data)\njim(x, y, gridded2, title=\"NUFFT gridding without DCF\")","category":"section"},{"location":"generated/mri/1-nufft/#Density-compensation","page":"NUFFT Overview","title":"Density compensation","text":"To get a decent image with NUFFT-based gridding of non-Cartesian data, one must compensate for the k-space sampling density. See this book chapter for details.\n\nBecause this example uses radial sampling, we can borrow ideas from tomography, especially the ramp filter, to define a reasonable density compensation function (DCF).\n\nHere is a basic DCF version that uses the ramp filter in a simple way, corresponding to the areas of annular segments (Voronoi cells in polar coordinates). The dcf .* data line uses Julia's broadcast feature to apply the 1D DCF to each radial spoke.\n\ndν = 1/FOV # k-space radial sample spacing\ndcf = pi / Nϕ * dν * abs.(kr) # (N+1) weights along k-space polar coordinate\ndcf = dcf / oneunit(eltype(dcf)) # kludge: units not working with LinearMap now\ngridded3 = A' * vec(dcf .* data)\np3 = jim(x, y, gridded3, title=\"NUFFT gridding with simple ramp-filter DCF\"; clim)\n\nThe image is more reasonable than without any DCF, but we can do better (quantitatively) using the correction of Lauzon&Rutt, 1996 and Joseph, 1998.\n\ndcf = pi / Nϕ * dν * abs.(kr) # see lauzon:96:eop, joseph:98:sei\ndcf[kr .== 0/mm] .= pi * (dν/2)^2 / Nϕ # area of center disk\ndcf = dcf / oneunit(eltype(dcf)) # kludge: units not working with LinearMap now\ngridded4 = A' * vec(dcf .* data)\np4 = jim(x, y, gridded4, title=\"NUFFT gridding with better ramp-filter DCF\"; clim)\n\nA profile helps illustrate the improvement.\n\npp = plot(x, real(gridded4[:,N÷2]), label=\"Modified ramp DCF\")\nplot!(x, real(gridded3[:,N÷2]), label=\"Basic ramp DCF\")\nplot!(x, real(ideal[:,N÷2]), label=\"Ideal\", xlabel=L\"x\", ylabel=\"middle profile\")\n\nisinteractive() && prompt();\nnothing #hide\n\nFinally we have made a NUFFT gridded image with DCF that has the appropriate range of values, but it still looks less than ideal. So next we try an iterative approach.","category":"section"},{"location":"generated/mri/1-nufft/#Iterative-MR-image-reconstruction-using-NUFFT","page":"NUFFT Overview","title":"Iterative MR image reconstruction using NUFFT","text":"As an initial iterative approach, let's apply a few iterations of conjugate gradient (CG) to seek the minimizer of the least-squares cost function:\n\narg min_x frac12  A x - y _2^2\n\nCG is well-suited to minimizing quadratic cost functions, but we do not expect the image to be great quality because radial sampling omits the \"corners\" of k-space so the NUFFT operator A is badly conditioned.\n\nThere is a subtle point here about dx when converting the Fourier integral to a sum. Here y is data/dx^2.\n\ngradf = u -> u - vec(data/dx^2) # gradient of f(u) = 1/2 \\| u - y \\|^2\ncurvf = u -> 1 # curvature of f(u)\nx0 = gridded4 # initial guess: best gridding reconstruction\nxls, _ = ncg([A], [gradf], [curvf], x0; niter = 20)\np5 = jim(x, y, xls, \"|LS-CG reconstruction|\"; clim)","category":"section"},{"location":"generated/mri/1-nufft/#Regularized-MR-image-reconstruction","page":"NUFFT Overview","title":"Regularized MR image reconstruction","text":"To improve the results, we include regularization. Here we would like to reconstruct an image by finding the minimizer of a regularized LS cost function such as the following:\n\narg min_x frac12  A x - y _2^2 + beta R(x)\n qquad\nR(x) = 1 psi(T x)","category":"section"},{"location":"generated/mri/1-nufft/#Tikhonov-regularization","page":"NUFFT Overview","title":"Tikhonov regularization","text":"The simplest option is Tikhonov regularization, where R(x) = (β_02)  x _2^2 corresponding to T = I and ψ(z) = (β_02)  z ^2 above.\n\nβ₀ = 1e-0\nxtik, _ = ncg([A, sqrt(β₀)*I], [gradf, x -> β₀*x], [curvf, x -> β₀], x0; niter = 80)\np6 = jim(x, y, xtik, \"|Tikhonov Regularized|\"; clim)\n\nComparing the error images with the same grayscale window, the regularized reconstruction has somewhat lower errors.\n\nelim = (0, 1)\necolor = :cividis\np5e = jim(x, y, abs.(xls - ideal), \"|LS-CG error|\"; clim=elim, color=ecolor)\np6e = jim(x, y, abs.(xtik - ideal), \"|Tik error|\"; clim=elim, color=ecolor)\nplot(p5e, p6e; size=(800,300))\n\nErrors in k-space\n\np5f = jimk(abs.(Ax_to_y(A * xls) - data) / dscale, \"|LS-CG kspace error|\")\np6f = jimk(abs.(Ax_to_y(A * xtik) - data) / dscale, \"|Tik. kspace error|\")\np56f = plot(p5f, p6f)","category":"section"},{"location":"generated/mri/1-nufft/#Edge-preserving-regularization","page":"NUFFT Overview","title":"Edge-preserving regularization","text":"Now consider edge-preserving regularization where T is a 2D finite-differencing operator and ψ is a potential function. This operator maps a NN image into a NN2 array with the horizontal and vertical finite differences.\n\nT = diffl_map((N,N), [1,2] ; T = ComplexF32)\n\nApplying this operator to the ideal image illustrated its action:\n\np7 = jim(x, y, T * ideal; nrow=1, size = (600, 300),\n title=\"Horizontal and vertical finite differences\")","category":"section"},{"location":"generated/mri/1-nufft/#Edge-preserving-regularization-2","page":"NUFFT Overview","title":"Edge-preserving regularization","text":"We use the Fair potential function: a rounded corner version of absolute value, an approximation to anisotropic total variation (TV).\n\nβ = 2^13 # regularization parameter\nδ = 0.05 # edge-preserving parameter\nwpot = z -> 1 / (1 + abs(z)/δ) # weighting function","category":"section"},{"location":"generated/mri/1-nufft/#Nonlinear-CG-algorithm","page":"NUFFT Overview","title":"Nonlinear CG algorithm","text":"We apply a (nonlinear) CG algorithm to seek the minimizer of the cost function. Nonlinear CG is well suited to convex problems that are locally quadratic like the regularized cost function considered here. See this survey paper for an overview of optimization methods for MRI.\n\nB = [A, T] # see MIRT.ncg\ngradf = [u -> u - vec(data/dx^2), # data-term gradient, correct for pixel area\n         u -> β * (u .* wpot.(u))] # regularizer gradient\ncurvf = [u -> 1, u -> β] # curvature of quadratic majorizers\nx0 = gridded4 # initial guess is best gridding reconstruction\nxhat, _ = ncg(B, gradf, curvf, x0; niter = 90)\np8 = jim(x, y, xhat, \"Iterative reconstruction\"; clim)\n\nCompare the error images:\n\np8e = jim(x, y, abs.(xhat - ideal), \"|Reg. error|\"; clim=elim, color=ecolor)\np568e = plot(p5e, p6e, p8e; layout=(1,3), size=(1200,300))\n\nHere is a comparison of the profiles.\n\nplot!(pp, x, real(xls[:,N÷2]), label=\"LS-CG\")\nplot!(pp, x, real(xhat[:,N÷2]), label=\"Iterative edge-preserving\", color=:black)\n\nisinteractive() && prompt();\nnothing #hide\n\nIn this case, iterative image reconstruction provides the best looking image. One might argue this simulation was doomed to succeed, because the phantom is piece-wise constant, which is the best case for edge-preserving regularization. On the other hand, this was not an inverse crime (see also here) because the k-space data came from the analytical spectrum of ellipses, rather than from a discrete image.","category":"section"},{"location":"generated/mri/1-nufft/#Caveats","page":"NUFFT Overview","title":"Caveats","text":"The phantom used here was real-valued, which is unrealistic (although the reconstruction methods did not \"know\" it was real).\nThis simulation is for a single-coil scan whereas modern MRI scanners generally use multiple receive coils.\nThere was no statistical noise in this simulation.","category":"section"},{"location":"generated/mri/1-nufft/#Reproducibility","page":"NUFFT Overview","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#JuliaImageRecon-Examples","page":"Home","title":"JuliaImageRecon Examples","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"https://github.com/JuliaImageRecon/Examples\n\nImage reconstruction examples in the Julia language, compiled using Literate and Documenter.","category":"section"},{"location":"#Getting-started-with-Julia","page":"Home","title":"Getting started with Julia","text":"Install Julia from https://julialang.org\nLaunch the Julia app should open a Julia REPL.\nTo develop code, select an editor, preferably with Julia integration, such as VSCode or vim perhaps with tmux. Appropriate editor plug-ins are needed to use LaTeX-like tab-completion of unicode characters like ÷ ⊗ ⊕ ∘ × and α β γ.\nPeruse the reconstruction examples listed in the menu here. If your browser window is wide enough, you should see a menu to the left. If your window is narrow, you should see a hamburger menu button that will toggle open the examples menu sidebar.\nView the excellent documentation at JuliaImages\nCheck out some Julia tutorials, especially the one titled \"Just the Julia you need to get started in Data Science and ML\" by Raj Rao.","category":"section"},{"location":"#Getting-started-with-Julia-image-reconstruction","page":"Home","title":"Getting started with Julia image reconstruction","text":"These examples show you Julia code and the corresponding output in an HTML format suitable for viewing in a web browser without installing any software.\n\nYou could cut and paste portions of that Julia code into the Julia REPL, but that becomes tedious. Instead, click on the \"Edit on GitHub\" link (in the upper right, with github icon), where you can then download the entire Julia code file that generated any of these examples.\n\nFor example, the code for the NUFFT example is at this url. After downloading such a file such as 1-nufft.jl, you can run it by typing include(\"1-nufft.jl\") at the Julia REPL.","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#4-cs-sense-2d","page":"Compressed Sensing 2D pMRI ","title":"Compressed Sensing 2D pMRI ","text":"This example illustrates how to perform 2D compressed sensing image reconstruction from Cartesian sampled MRI data for parallel MRI (sensitivity encoding) with 1-norm regularization of orthogonal wavelet coefficients, using the Julia language.\n\nThis page comes from a single Julia file: 4-cs-sense-2d.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 4-cs-sense-2d.ipynb, or open it in binder here: 4-cs-sense-2d.ipynb.\n\nThis demo is somewhat similar to Fig. 3 in the survey paper \"Optimization methods for MR image reconstruction,\" in Jan 2020 IEEE Signal Processing Magazine, except\n\nthe sampling is 1D phase encoding instead of 2D,\nthere are multiple coils,\nwe use units (WIP - todo)\nthe simulation avoids inverse crimes.\n\nPackages used in this demo (run Pkg.add as needed):\n\nusing ImagePhantoms: ellipse_parameters, SheppLoganBrainWeb, ellipse, phantom\nusing ImagePhantoms: mri_smap_fit, mri_spectra\n#using ImageFiltering: imfilter, centered\nusing ImageMorphology: dilate #, label_components # imfill\nusing LazyGrids: ndgrid\nusing ImageGeoms: embed, embed!\nusing MIRT: Aodwt, Asense\nusing MIRTjim: jim, prompt\nusing MIRT: ir_mri_sensemap_sim\nusing MIRT: pogm_restart\nusing LinearAlgebra: norm, dot\nusing LinearMapsAA: LinearMapAA\nusing Plots; default(markerstrokecolor=:auto, label=\"\")\nusing FFTW: fft!, bfft!, fftshift!\nusing Random: seed!\nusing InteractiveUtils: versioninfo\n\nThe following line is helpful when running this jl-file as a script; this way it will prompt user to hit a key after each image is displayed.\n\nisinteractive() && jim(:prompt, true);\nnothing #hide","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#Create-(synthetic)-data","page":"Compressed Sensing 2D pMRI ","title":"Create (synthetic) data","text":"Image geometry:\n\nuu = 1\nfovs = (256, 256) .* uu\nnx, ny = (192, 256)\ndx, dy = fovs ./ (nx,ny)\nx = (-(nx÷2):(nx÷2-1)) * dx\ny = (-(ny÷2):(ny÷2-1)) * dy\n\nModified Shepp-Logan phantom with random complex phases per ellipse\n\nobject = ellipse_parameters(SheppLoganBrainWeb() ; disjoint=true, fovs)\nseed!(0)\nobject = vcat( (object[1][1:end-1]..., 1), # random phases\n    [(ob[1:end-1]..., randn(ComplexF32)) for ob in object[2:end]]...)\nobject = ellipse(object)\noversample = 3\nXtrue = phantom(x, y, object, oversample)\ncfun = z -> cat(dims = ndims(z)+1, real(z), imag(z))\njim(:aspect_ratio, :equal)\njim(x, y, cfun(Xtrue), \"True image\\n real | imag\"; ncol=2)\n\nMask (support for image reconstruction)\n\nmask = abs.(Xtrue) .> 0\nmask = dilate(dilate(dilate(mask))) # repeated dilate with 3×3 square\n@assert mask .* Xtrue == Xtrue\njim(x, y, mask + abs.(Xtrue), \"Mask + |Xtrue|\")\n\nMake sensitivity maps, normalized so SSoS = 1:\n\nncoil = 4\nsmap_raw = ir_mri_sensemap_sim(dims=(nx,ny); ncoil, orbit_start=[45])\njif(args...; kwargs...) = jim(args...; prompt=false, kwargs...)\np1 = jif(x, y, smap_raw, \"Sensitivity maps raw\");\n\nsum_last = (f, x) -> selectdim(sum(f, x; dims=ndims(x)), ndims(x), 1)\nssos_fun = smap -> sqrt.(sum_last(abs2, smap)) # SSoS\nssos_raw = ssos_fun(smap_raw) # SSoS raw\np2 = jif(x, y, ssos_raw, \"SSoS raw, ncoil=$ncoil\");\n\nsmap = @. smap_raw / ssos_raw * mask # normalize and mask\nssos = ssos_fun(smap) # SSoS\n@assert all(≈(1), @view ssos[mask]) # verify ≈ 1\nsmaps = [eachslice(smap; dims = ndims(smap))...] # stack\np3 = jif(x, y, smaps, \"|Sensitivity maps normalized|\")\np4 = jif(x, y, map(x -> angle.(x), smaps), \"∠Sensitivity maps\"; color=:hsv)\njim(p1, p2, p3, p4)\n\nFrequency sample vectors; crucial to match mri_smap_basis internals!\n\nfx = (-(nx÷2):(nx÷2-1)) / (nx*dx)\nfy = (-(ny÷2):(ny÷2-1)) / (ny*dy)\ngx, gy = ndgrid(fx, fy);\nnothing #hide\n\nSomewhat random 1D phase-encode sampling:\n\nseed!(0); sampfrac = 0.3; samp = rand(ny÷2) .< sampfrac\ntmp = rand(ny÷2) .< 0.5; samp = [samp .* tmp; reverse(samp .* .!tmp)] # symmetry\nsamp .|= (abs.(fy*dy) .< 1/8) # fully sampled center ±1/8 phase-encodes\nny_count = count(samp)\nsamp = trues(nx) * samp'\nsamp_frac = round(100*count(samp) / (nx*ny), digits=2)\njim(fx, fy, samp, title=\"k-space sampling ($ny_count / $ny = $samp_frac%)\")\n\nTo avoid an inverse crime, here we use the 2012 method of Guerquin-Kern et al. and use the analytical k-space values of the phantom combined with an analytical model for the sensitivity maps.\n\nkmax = 7\nfit = mri_smap_fit(smaps, embed; mask, kmax, deltas=(dx,dy))\njim(\n jif(x, y, cfun(smaps), \"Original maps\"; clim=(-1,1), nrow=4),\n jif(x, y, cfun(fit.smaps), \"Fit maps\"; clim=(-1,1), nrow=4),\n jif(x, y, cfun(100 * (fit.smaps - smaps)), \"error * 100\"; nrow=4),\n layout = (1,3),\n)\n\nAnalytical spectra computation for complex phantom using all smaps. (No inverse crime here.)\n\nytrue = mri_spectra(gx[samp], gy[samp], object, fit)\nytrue = hcat(ytrue...)\nytrue = ComplexF32.(ytrue) # save memory\nsize(ytrue)\n\nNoisy under-sampled k-space data:\n\nsig = 1\nydata = ytrue + oneunit(eltype(ytrue)) *\n    sig * √(2f0) * randn(ComplexF32, size(ytrue)) # complex noise with units!\nysnr = 20 * log10(norm(ytrue) / norm(ydata - ytrue)) # data SNR in dB\n\nDisplay zero-filled data:\n\nlogger = (x; min=-6, up=maximum(abs,x)) -> log10.(max.(abs.(x) / up, (10.)^min))\njim(:abswarn, false) # suppress warnings about showing magnitude\ntmp = embed(ytrue[:,1],samp)\njim(\n jif(fx, fy, logger(tmp),\n    title=\"k-space |data|\\n(zero-filled, coil 1)\",\n    xlabel=\"νx\", ylabel=\"νy\"),\n jif(fx, fy, angle.(tmp),\n    title=\"∠data, coil 1\"; color=:hsv)\n)","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#Prepare-to-reconstruct","page":"Compressed Sensing 2D pMRI ","title":"Prepare to reconstruct","text":"Create a system matrix (encoding matrix) and an initial image.\n\nThe system matrix is a LinearMapAO object, akin to a fatrix in Matlab MIRT.\n\nThis system model (\"encoding matrix\") is for a 2D image x being mapped to an array of size count(samp) × ncoil k-space data.\n\nHere we construct it from FFT and coil map components. So this is like \"seeing the sausage being made.\"\n\nThe dx * dy scale factor here is required because the true k-space data ytrue comes from an analytical Fourier transform, but the reconstruction uses a discrete Fourier transform. This factor is also needed from a unit-balance perspective.\n\nAscale = Float32(dx * dy)\nA = Ascale * Asense(samp, smaps) # operator!\n(size(A), A._odim, A._idim)\n\nCompare the analytical k-space data with the discrete model k-space data:\n\ny0 = embed(ytrue, samp) # analytical\ny1 = embed(A * Xtrue, samp) # discrete\njim(\n jif(logger(y0; up=maximum(abs,y0)), \"analytical\"; clim=(-6,0)),\n jif(logger(y1; up=maximum(abs,y0)), \"discrete\"; clim=(-6,0)),\n jif(logger(y1 - y0; up=maximum(abs,y0)), \"difference\"),\n)\nnorm(y1) / norm(y0) # scale factor is ≈1\n\nInitial image based on zero-filled adjoint reconstruction. Note the (dx*dy)² scale factor here!\n\nnrmse = (x) -> round(norm(x - Xtrue) / norm(Xtrue) * 100, digits=1)\nX0 = 1.0f0/(nx*ny) * (A' * ydata) / (dx*dy)^2\njim(x, y, X0, \"|X0|: initial image; NRMSE $(nrmse(X0))%\")","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#Wavelet-sparsity-in-synthesis-form","page":"Compressed Sensing 2D pMRI ","title":"Wavelet sparsity in synthesis form","text":"The image reconstruction optimization problem here is\n\narg min_x\nfrac12  A x - y _2^2 + beta   W x _1\n\nwhere\n\ny is the k-space data,\nA is the system model (simply Fourier encoding F here),\nW is an orthogonal discrete (Haar) wavelet transform, again implemented as a LinearMapAA object.\n\nBecause W is unitary, we make the change of variables z = W x and solve for z and then let x = W z at the end. In fact we use a weighted 1-norm where only the detail wavelet coefficients are regularized, not the approximation coefficients.\n\nOrthogonal discrete wavelet transform operator (LinearMapAO):\n\nW, scales, _ = Aodwt((nx,ny) ; T = eltype(A))\nisdetail = scales .> 0\njim(\n jif(scales, \"wavelet scales\"),\n jif(real(W * Xtrue) .* isdetail, \"wavelet detail coefficients\\nreal for Xtrue\"),\n)\n\nInputs needed for proximal gradient methods. The trickiest part of this is determining a bound on the Lipschitz constant, i.e., the spectral norm of (AW)(AW), which is the same as the spectral norm of AA because W is unitary. Here we have SSOS=1 for the coil maps, so we just need to account for the number of voxels (because fft & bfft are not the unitary DFT) and for the scale factor. (If SSOS was nonuniform, we would use eqn. (6) of Matt Muckley's BARISTA paper. Submit an issue if you need an example using that.)\n\nf_Lz = Ascale^2 * nx*ny # Lipschitz constant\nAz = A * W' # another operator!\nFnullz = (z) -> 0 # cost function in `z` not needed\nf_gradz = (z) -> Az' * (Az * z - ydata)\nregz = 0.03 * nx * ny # oracle from Xtrue wavelet coefficients!\ncostz = (z) -> 1/2 * norm(Az * z - ydata)^2 + regz * norm(z,1) # 1-norm regularizer\nsoft = (z,c) -> sign(z) * max(abs(z) - c, 0) # soft thresholding\ng_prox = (z,c) -> soft.(z, isdetail .* (regz * c)) # proximal operator (shrink details only)\nz0 = W * X0\njim(\n jif(z0, \"|wavelet coefficients|\"),\n jif(z0 .* isdetail, \"|detail coefficients|\"),\n ; plot_title = \"Initial\",\n)","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#Iterate","page":"Compressed Sensing 2D pMRI ","title":"Iterate","text":"Run ISTA=PGM and FISTA=FPGM and POGM, the latter two with adaptive restart. See Kim & Fessler, 2018 for adaptive restart algorithm details.\n\nFunctions for tracking progress:\n\nfunction fun_ista(iter, xk_z, yk, is_restart)\n    xh = W' * xk_z\n    return (costz(xk_z), nrmse(xh), is_restart) # , psnr(xh) # time()\nend\n\nfunction fun_fista(iter, xk, yk_z, is_restart)\n    xh = W' * yk_z\n    return (costz(yk_z), nrmse(xh), is_restart) # , psnr(xh) # time()\nend;\nnothing #hide\n\nRun and compare three proximal gradient methods:\n\nniter = 20\nz_ista, out_ista = pogm_restart(z0, Fnullz, f_gradz, f_Lz;\n    mom=:pgm, niter,\n    restart=:none, restart_cutoff=0., g_prox, fun=fun_ista)\nXista = W'*z_ista\n@show nrmse(Xista)\n\nz_fista, out_fista = pogm_restart(z0, Fnullz, f_gradz, f_Lz;\n    mom=:fpgm, niter,\n    restart=:gr, restart_cutoff=0., g_prox, fun=fun_fista)\nXfista = W'*z_fista\n@show nrmse(Xfista)\n\nz_pogm, out_pogm = pogm_restart(z0, Fnullz, f_gradz, f_Lz;\n    mom=:pogm, niter,\n    restart=:gr, restart_cutoff=0., g_prox, fun=fun_fista)\nXpogm = W'*z_pogm\n@show nrmse(Xpogm)\n\njim(\n jif(x, y, Xfista, \"FISTA/FPGM\"),\n jif(x, y, Xpogm, \"POGM with ODWT\"),\n)","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#Convergence-rate:-POGM-is-fastest","page":"Compressed Sensing 2D pMRI ","title":"Convergence rate: POGM is fastest","text":"Plot cost function vs iteration:\n\ncost_ista = [out_ista[k][1] for k in 1:niter+1]\ncost_fista = [out_fista[k][1] for k in 1:niter+1]\ncost_pogm = [out_pogm[k][1] for k in 1:niter+1]\ncost_min = min(minimum(cost_ista), minimum(cost_pogm))\npc = plot(xlabel=\"iteration k\", ylabel=\"Relative cost\")\nscatter!(0:niter, cost_ista  .- cost_min, label=\"Cost ISTA\")\nscatter!(0:niter, cost_fista .- cost_min, markershape=:square, label=\"Cost FISTA\")\nscatter!(0:niter, cost_pogm  .- cost_min, markershape=:utriangle, label=\"Cost POGM\")\n\nisinteractive() && prompt();\nnothing #hide\n\nPlot NRMSE vs iteration:\n\nnrmse_ista = [out_ista[k][2] for k in 1:niter+1]\nnrmse_fista = [out_fista[k][2] for k in 1:niter+1]\nnrmse_pogm = [out_pogm[k][2] for k in 1:niter+1]\npn = plot(xlabel=\"iteration k\", ylabel=\"NRMSE %\")#, ylims=(3,6.5))\nscatter!(0:niter, nrmse_ista, label=\"NRMSE ISTA\")\nscatter!(0:niter, nrmse_fista, markershape=:square, label=\"NRMSE FISTA\")\nscatter!(0:niter, nrmse_pogm, markershape=:utriangle, label=\"NRMSE POGM\")\n\nisinteractive() && prompt();\nnothing #hide\n\nShow error images:\n\nsnrfun = (x) -> round(-20log10(nrmse(x)/100); digits=1)\np1 = jif(x, y, Xtrue, \"|true|\"; clim=(0,2.5))\np2 = jif(x, y, X0, \"|X0|: initial\"; clim=(0,2.5))\np3 = jif(x, y, Xpogm, \"|POGM recon|\"; clim=(0,2.5))\np5 = jif(x, y, X0 - Xtrue, \"|X0 error|\"; clim=(0,1), color=:cividis,\n   xlabel = \"NRMSE = $(nrmse(X0))%\\n SNR = $(snrfun(X0)) dB\")\np6 = jif(x, y, Xpogm - Xtrue, \"|Xpogm error|\"; clim=(0,1), color=:cividis,\n   xlabel = \"NRMSE = $(nrmse(Xpogm))%\\n SNR = $(snrfun(Xpogm)) dB\")\npe = jim(p2, p3, p5, p6)","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#Discussion","page":"Compressed Sensing 2D pMRI ","title":"Discussion","text":"As reported in the optimization survey paper cited above, POGM converges faster than ISTA and FISTA.\n\nThe final images serve as a reminder that NRMSE (and PSNR) and dubious image quality metrics. The NRMSE after 20 iterations may seem only a bit lower than the NRMSE of the initial image, but aliasing artifacts (ripples) were greatly reduced by the CS-SENSE reconstruction method.","category":"section"},{"location":"generated/mri/4-cs-sense-2d/#Reproducibility","page":"Compressed Sensing 2D pMRI ","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/mri/5-l-plus-s/#5-l-plus-s","page":"L+S 2D dynamic recon","title":"L+S 2D dynamic recon","text":"This page illustrates dynamic parallel MRI image reconstruction using a low-rank plus sparse (L+S) model optimized by a fast algorithm described in the paper by Claire Lin and Jeff Fessler Efficient Dynamic Parallel MRI Reconstruction for the Low-Rank Plus Sparse Model, IEEE Trans. on Computational Imaging, 5(1):17-26, 2019, by Claire Lin and Jeff Fessler, EECS Department, University of Michigan.\n\nThe Julia code here is a translation of part of the Matlab code used in the original paper.\n\nIf you use this code, please cite that paper.\n\nThis page comes from a single Julia file: 5-l-plus-s.jl.\n\nYou can access the source code for such Julia documentation using the 'Edit on GitHub' link in the top right. You can view the corresponding notebook in nbviewer here: 5-l-plus-s.ipynb, or open it in binder here: 5-l-plus-s.ipynb.","category":"section"},{"location":"generated/mri/5-l-plus-s/#Setup","page":"L+S 2D dynamic recon","title":"Setup","text":"Packages needed here.\n\n# using Unitful: s\nusing Plots; cgrad, default(markerstrokecolor=:auto, label=\"\")\nusing MIRT: Afft, Asense, embed\nusing MIRT: pogm_restart, poweriter\nusing MIRTjim: jim, prompt\nusing FFTW: fft!, bfft!, fftshift!\nusing LinearMapsAA: LinearMapAA, block_diag, redim, undim\nusing MAT: matread\nimport Downloads # todo: use Fetch or DataDeps?\nusing LinearAlgebra: dot, norm, svd, svdvals, Diagonal, I\nusing Random: seed!\nusing Statistics: mean\nusing LaTeXStrings\n\nThe following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.\n\njif(args...; kwargs...) = jim(args...; prompt=false, kwargs...)\nisinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"section"},{"location":"generated/mri/5-l-plus-s/#Overview","page":"L+S 2D dynamic recon","title":"Overview","text":"Dynamic image reconstruction using a \"low-rank plus sparse\" or \"L+S\" approach was proposed by Otazo et al. and uses the following cost function:\n\nX = hatL + hatS\nqquad\n(hatL hatS)\n= arg min_LS frac12  E (L + S) - d _2^2\n + λ_L  L _*\n + λ_S  vec(T S) _1\n\nwhere T is a temporal unitary FFT, E is an encoding operator (system matrix), and d is Cartesian undersampled multicoil k-space data.\n\nThe Otazo paper used an iterative soft thresholding algorithm (ISTA) to solve this optimization problem. Using FISTA is faster, but using the proximal optimized gradient method (POGM) with adaptive restart is even faster.\n\nThis example reproduces part of Figures 1 & 2 in Claire Lin's paper, based on the cardiac perfusion example.","category":"section"},{"location":"generated/mri/5-l-plus-s/#Read-data","page":"L+S 2D dynamic recon","title":"Read data","text":"if !@isdefined(data)\n    url = \"https://github.com/JeffFessler/MIRTdata/raw/main/mri/lin-19-edp/\"\n    dataurl = url * \"cardiac_perf_R8.mat\"\n    data = matread(Downloads.download(dataurl))\n    xinfurl = url * \"Xinf.mat\"\n    Xinf = matread(Downloads.download(xinfurl))[\"Xinf\"][\"perf\"] # (128,128,40)\nend;\nnothing #hide\n\nShow converged image as a preview:\n\npinf = jim(Xinf, L\"\\mathrm{Converged\\ image\\ sequence } X_∞\")\n\nOrganize k-space data:\n\nif !@isdefined(ydata0)\n    ydata0 = data[\"kdata\"] # k-space data full of zeros\n    ydata0 = permutedims(ydata0, [1, 2, 4, 3]) # (nx,ny,nc,nt)\n    ydata0 = ComplexF32.(ydata0)\nend\n(nx, ny, nc, nt) = size(ydata0)\n\nExtract sampling pattern from zeros of k-space data:\n\nif !@isdefined(samp)\n    samp = ydata0[:,:,1,:] .!= 0\n    for ic in 2:nc # verify it is same for all coils\n        @assert samp == (ydata0[:,:,ic,:] .!= 0)\n    end\n    kx = -(nx÷2):(nx÷2-1)\n    ky = -(ny÷2):(ny÷2-1)\n    psamp = jim(kx, ky, samp, \"Sampling patterns for $nt frames\";\n       xlabel=L\"k_x\", ylabel=L\"k_y\")\nend\n\nAre all k-space rows are sampled in one of the 40 frames? Sadly no. The 10 blue rows shown below are never sampled. A better sampling pattern design could have avoided this issue.\n\nsamp_sum = sum(samp, dims=3)\ncolor = cgrad([:blue, :black, :white], [0, 1/2nt, 1])\npssum = jim(kx, ky, samp_sum; xlabel=\"kx\", ylabel=\"ky\",\n    color, clim=(0,nt), title=\"Number of sampled frames out of $nt\")\n\nPrepare coil sensitivity maps\n\nif !@isdefined(smaps)\n    smaps_raw = data[\"b1\"] # raw coil sensitivity maps\n    jim(smaps_raw, \"Raw |coil maps| for $nc coils\")\n    sum_last = (f, x) -> selectdim(sum(f, x; dims=ndims(x)), ndims(x), 1)\n    ssos_fun = smap -> sqrt.(sum_last(abs2, smap)) # SSoS\n    ssos_raw = ssos_fun(smaps_raw)\n    smaps = smaps_raw ./ ssos_raw\n    ssos = ssos_fun(smaps)\n    @assert all(≈(1), ssos)\n    pmap = jim(smaps, \"Normalized |coil maps| for $nc coils\")\nend\n\nTemporal unitary FFT sparsifying transform for image sequence of size (nx, ny, nt):\n\nTF = Afft((nx,ny,nt), 3; unitary=true) # unitary FFT along 3rd (time) dimension\nif false # verify adjoint\n    tmp1 = randn(ComplexF32, nx, ny, nt)\n    tmp2 = randn(ComplexF32, nx, ny, nt)\n    @assert dot(tmp2, TF * tmp1) ≈ dot(TF' * tmp2, tmp1)\n    @assert TF' * (TF * tmp1) ≈ tmp1\n    (size(TF), TF._odim, TF._idim)\nend\n\nExamine temporal Fourier sparsity of Xinf. The low temporal frequencies dominate, as expected, because Xinf was reconstructed using this regularizer!\n\ntmp = TF * Xinf\nptfft = jim(tmp, \"|Temporal FFT of Xinf|\")","category":"section"},{"location":"generated/mri/5-l-plus-s/#System-matrix","page":"L+S 2D dynamic recon","title":"System matrix","text":"Construct dynamic parallel MRI system model. It is block diagonal where each frame has its own sampling pattern. The input (image) here has size (nx=128, ny=128, nt=40). The output (data) has size (nsamp=2048, nc=12, nt=40) because every frame has 16 phase-encode lines of 128 samples.\n\ntodo: precompute (i)fft along readout direction to save time\n\nThe code in the original Otazo et al. paper used an ifft in the forward model and an fft in the adjoint, so we must use a flag here to match that model.\n\nAotazo = (samp, smaps) -> Asense(samp, smaps; unitary=true, fft_forward=false) # Otazo style\nA = block_diag([Aotazo(s, smaps) for s in eachslice(samp, dims=3)]...)\n#A = ComplexF32(1/sqrt(nx*ny)) * A # match Otazo's scaling\n(size(A), A._odim, A._idim)\n\nReshape data to match the system model\n\nif !@isdefined(ydata)\n    tmp = reshape(ydata0, :, nc, nt)\n    tmp = [tmp[vec(s),:,it] for (it,s) in enumerate(eachslice(samp, dims=3))]\n    ydata = cat(tmp..., dims=3) # (nsamp,nc,nt) = (2048,12,40) no \"zeros\"\nend\nsize(ydata)\n\nFinal encoding operator E for L+S because we stack X = [L;S]\n\ntmp = LinearMapAA(I(nx*ny*nt);\n    odim=(nx,ny,nt), idim=(nx,ny,nt), T=ComplexF32, prop=(;name=\"I\"))\ntmp = kron([1 1], tmp)\nAII = redim(tmp; odim=(nx,ny,nt), idim=(nx,ny,nt,2)) # \"squeeze\" odim\nE = A * AII;\nnothing #hide\n\nRun power iteration to verify that opnorm(E) = √2\n\nif false\n    (_, σ1E) = poweriter(undim(E)) # 1.413 ≈ √2\nelse\n    σ1E = √2\nend\n\nCheck scale factor of Xinf. (It should be ≈1.)\n\ntmp = A * Xinf\nscale0 = dot(tmp, ydata) / norm(tmp)^2 # 1.009 ≈ 1\n\nCrude initial image\n\nL0 = A' * ydata # adjoint (zero-filled)\nS0 = zeros(ComplexF32, nx, ny, nt)\nX0 = cat(L0, S0, dims=ndims(L0)+1) # (nx, ny, nt, 2) = (128, 128, 40, 2)\nM0 = AII * X0 # L0 + S0\npm0 = jim(M0, \"|Initial L+S via zero-filled recon|\")","category":"section"},{"location":"generated/mri/5-l-plus-s/#LS-reconstruction","page":"L+S 2D dynamic recon","title":"L+S reconstruction","text":"Prepare for proximal gradient methods\n\nScalars to match Otazo's results\n\nscaleL = 130 / 1.2775 # Otazo's stopping St(1) / b1 constant squared\nscaleS = 1 / 1.2775; # 1 / b1 constant squared\nnothing #hide\n\nL+S regularizer\n\nlambda_L = 0.01 # regularization parameter\nlambda_S = 0.01 * scaleS\nLpart = X -> selectdim(X, ndims(X), 1) # extract \"L\" from X\nSpart = X -> selectdim(X, ndims(X), 2) # extract \"S\" from X\nnucnorm(L::AbstractMatrix) = sum(svdvals(L)) # nuclear norm\nnucnorm(L::AbstractArray) = nucnorm(reshape(L, :, nt)); # (nx*ny, nt) for L\nnothing #hide\n\nOptimization overall composite cost function\n\nFcost = X -> 0.5 * norm(E * X - ydata)^2 +\n    lambda_L * scaleL * nucnorm(Lpart(X)) + # note scaleL !\n    lambda_S * norm(TF * Spart(X), 1);\n\nf_grad = X -> E' * (E * X - ydata); # gradient of data-fit term\n\nLipschitz constant of data-fit term is 2 because A is unitary and AII is like ones(2,2).\n\nf_L = 2; # σ1E^2\nnothing #hide\n\nProximal operator for scaled nuclear norm β  X _*: singular value soft thresholding (SVST).\n\nfunction SVST(X::AbstractArray, β)\n    dims = size(X)\n    X = reshape(X, :, dims[end]) # assume time frame is the last dimension\n    U,s,V = svd(X)\n    sthresh = @. max(s - β, 0)\n    keep = findall(>(0), sthresh)\n    X = U[:,keep] * Diagonal(sthresh[keep]) * V[:,keep]'\n    X = reshape(X, dims)\n    return X\nend;\nnothing #hide\n\nCombine proximal operators for L and S parts to make overall prox for X\n\nsoft = (v,c) -> sign(v) * max(abs(v) - c, 0) # soft threshold function\nS_prox = (S, β) -> TF' * soft.(TF * S, β) # 1-norm proximal mapping for unitary TF\ng_prox = (X, c) -> cat(dims=ndims(X),\n    SVST(Lpart(X), c * lambda_L * scaleL),\n    S_prox(Spart(X), c * lambda_S),\n);\n\nif false # check functions\n    @assert Fcost(X0) isa Real\n    tmp = f_grad(X0)\n    @assert size(tmp) == size(X0)\n    tmp = SVST(Lpart(X0), 1)\n    @assert size(tmp) == size(L0)\n    tmp = S_prox(S0, 1)\n    @assert size(tmp) == size(S0)\n    tmp = g_prox(X0, 1)\n    @assert size(tmp) == size(X0)\nend\n\n\nniter = 10\nfun = (iter, xk, yk, is_restart) -> (Fcost(xk), xk); # logger\nnothing #hide\n\nRun PGM\n\nif !@isdefined(Mpgm)\n    f_mu = 2/0.99 - f_L # trick to match 0.99 step size in Lin 1999\n    f_mu = 0\n    xpgm, out_pgm = pogm_restart(X0, (x) -> 0, f_grad, f_L ;\n        f_mu, mom = :pgm, niter, g_prox, fun)\n    Mpgm = AII * xpgm\nend;\nnothing #hide\n\nRun FPGM (FISTA)\n\nif !@isdefined(Mfpgm)\n    xfpgm, out_fpgm = pogm_restart(X0, (x) -> 0, f_grad, f_L ;\n        mom = :fpgm, niter, g_prox, fun)\n    Mfpgm = AII * xfpgm\nend;\nnothing #hide\n\nRun POGM\n\nif !@isdefined(Mpogm)\n    xpogm, out_pogm = pogm_restart(X0, (x) -> 0, f_grad, f_L ;\n        mom = :pogm, niter, g_prox, fun)\n    Mpogm = AII * xpogm\nend;\nnothing #hide\n\nLook at final POGM image components\n\npx = jim(\n jif(Lpart(xpogm), \"L\"),\n jif(Spart(xpogm), \"S\"),\n jif(Mpogm, \"M=L+S\"),\n jif(Xinf, \"Minf\"),\n)\n\nPlot cost function\n\ncosts = out -> [o[1] for o in out]\nnrmsd = out -> [norm(AII*o[2]-Xinf)/norm(Xinf) for o in out]\ncost_pgm = costs(out_pgm)\ncost_fpgm = costs(out_fpgm)\ncost_pogm = costs(out_pogm)\npc = plot(xlabel = \"iteration\", ylabel = \"cost\")\nplot!(0:niter, cost_pgm, marker=:circle, label=\"PGM (ISTA)\")\nplot!(0:niter, cost_fpgm, marker=:square, label=\"FPGM (FISTA)\")\nplot!(0:niter, cost_pogm, marker=:star, label=\"POGM\")\n\nPlot NRMSD vs Matlab Xinf\n\nnrmsd_pgm = nrmsd(out_pgm)\nnrmsd_fpgm = nrmsd(out_fpgm)\nnrmsd_pogm = nrmsd(out_pogm)\npd = plot(xlabel = \"iteration\", ylabel = \"NRMSD vs Matlab Xinf\")\nplot!(0:niter, nrmsd_pgm, marker=:circle, label=\"PGM (ISTA)\")\nplot!(0:niter, nrmsd_fpgm, marker=:square, label=\"FPGM (FISTA)\")\nplot!(0:niter, nrmsd_pogm, marker=:star, label=\"POGM\")","category":"section"},{"location":"generated/mri/5-l-plus-s/#Discussion","page":"L+S 2D dynamic recon","title":"Discussion","text":"todo","category":"section"},{"location":"generated/mri/5-l-plus-s/#Reproducibility","page":"L+S 2D dynamic recon","title":"Reproducibility","text":"This page was generated with the following version of Julia:\n\nusing InteractiveUtils: versioninfo\nio = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')\n\nAnd with the following package versions\n\nimport Pkg; Pkg.status()\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
